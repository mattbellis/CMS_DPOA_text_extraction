{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Extraction from Physics Papers\n",
    "\n",
    "This notebook extracts dataset information from PDF papers using Claude AI.\n",
    "\n",
    "## Requirements\n",
    "```\n",
    "pip install anthropic pdfplumber pandas python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit these settings for your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Edit these values for your setup\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    \"input_folder\": r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "    \"output_file\": \"dataset_info.csv\",\n",
    "    \"model\": \"claude-sonnet-4-20250514\",\n",
    "    \"max_pages\": None,  # Set to a number to limit pages per PDF, None for all pages\n",
    "    \"verbose\": True  # Set to False for less logging\n",
    "}\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import anthropic\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "log_level = logging.DEBUG if CONFIG[\"verbose\"] else logging.INFO\n",
    "logging.basicConfig(\n",
    "    level=log_level,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # This ensures logging reconfigures in Jupyter\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for Claude to extract dataset information\n",
    "EXTRACTION_PROMPT = \"\"\"You are a scientific data extraction assistant specializing in high-energy physics papers.\n",
    "\n",
    "Analyze the following physics paper text and extract ALL dataset information mentioned. Focus on:\n",
    "- CMS Open Data datasets\n",
    "- Monte Carlo simulation samples\n",
    "- Real collision data samples\n",
    "- Any datasets with DOIs or official citation paths\n",
    "\n",
    "For EACH dataset found, extract these fields (use \"null\" if not available):\n",
    "1. dataset_name: The name or identifier of the dataset\n",
    "2. dataset_type: \"Real Data\" or \"Simulated MC\" \n",
    "3. official_path: The official citation path (e.g., /Jet/Run2010B-Apr21ReReco-v1/AOD)\n",
    "4. events_total: Total number of events in the dataset\n",
    "5. events_used: Number of events actually used in the analysis\n",
    "6. collision_energy_tev: Center-of-mass energy in TeV\n",
    "7. generator: MC generator used (e.g., Pythia, Madgraph) or \"N/A (Real Data)\"\n",
    "8. doi: The DOI identifier (just the DOI, not the full URL)\n",
    "9. size_bytes: Dataset size in bytes (convert from TB, GB, MB if needed: 1TB=1e12, 1GB=1e9, 1MB=1e6). If multiple sizes given (e.g., raw vs compressed), use the original/raw size.\n",
    "10. notes: Any other important details (luminosity, run period, selection criteria, etc.)\n",
    "\n",
    "Return your response as a JSON array of objects. Each object represents one dataset.\n",
    "If the paper mentions multiple pT bins or variants of the same base dataset, list each separately.\n",
    "\n",
    "Example output format:\n",
    "[\n",
    "    {\n",
    "        \"dataset_name\": \"Jet Primary Dataset\",\n",
    "        \"dataset_type\": \"Real Data\",\n",
    "        \"official_path\": \"/Jet/Run2010B-Apr21ReReco-v1/AOD\",\n",
    "        \"events_total\": \"20022826\",\n",
    "        \"events_used\": \"768687\",\n",
    "        \"collision_energy_tev\": \"7\",\n",
    "        \"generator\": \"N/A (Real Data)\",\n",
    "        \"doi\": \"10.7483/OPENDATA.CMS.3S7F.2E9W\",\n",
    "        \"size_bytes\": \"2000000000000\",\n",
    "        \"notes\": \"Run 2010B, 31.8 pb-1 integrated luminosity, 2.0 TB original size\"\n",
    "    }\n",
    "]\n",
    "\n",
    "CRITICAL: Return ONLY the JSON array, no other text, no markdown formatting, no code blocks, no explanation. Just the raw JSON array starting with [ and ending with ].\n",
    "\n",
    "Paper text to analyze:\n",
    "---\n",
    "{paper_text}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Extraction prompt defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str, max_pages: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        max_pages: Maximum number of pages to extract (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as a string\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from: {pdf_path}\")\n",
    "    \n",
    "    text_parts = []\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pages_to_process = pdf.pages[:max_pages] if max_pages else pdf.pages\n",
    "            \n",
    "            for i, page in enumerate(pages_to_process):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_parts.append(f\"--- Page {i+1} ---\\n{page_text}\")\n",
    "                    \n",
    "                # Also try to extract tables as they often contain dataset info\n",
    "                tables = page.extract_tables()\n",
    "                for j, table in enumerate(tables):\n",
    "                    if table:\n",
    "                        table_text = \"\\n\".join([\"\\t\".join([str(cell) if cell else \"\" for cell in row]) for row in table])\n",
    "                        text_parts.append(f\"--- Table {j+1} on Page {i+1} ---\\n{table_text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(text_parts)\n",
    "        logger.info(f\"Extracted {len(full_text)} characters from {len(pages_to_process)} pages\")\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def clean_json_response(response_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up Claude's response to extract valid JSON.\n",
    "    \n",
    "    Args:\n",
    "        response_text: Raw response from Claude\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned JSON string\n",
    "    \"\"\"\n",
    "    # Remove any leading/trailing whitespace\n",
    "    response_text = response_text.strip()\n",
    "    \n",
    "    # Remove markdown code blocks (```json ... ``` or ``` ... ```)\n",
    "    if response_text.startswith(\"```\"):\n",
    "        lines = response_text.split(\"\\n\")\n",
    "        # Remove first line (```json or ```)\n",
    "        lines = lines[1:]\n",
    "        # Remove last line if it's ```\n",
    "        if lines and lines[-1].strip() == \"```\":\n",
    "            lines = lines[:-1]\n",
    "        response_text = \"\\n\".join(lines).strip()\n",
    "    \n",
    "    # Try to extract JSON array if there's surrounding text\n",
    "    if not response_text.startswith(\"[\"):\n",
    "        # Use regex to find JSON array\n",
    "        match = re.search(r'\\[.*\\]', response_text, re.DOTALL)\n",
    "        if match:\n",
    "            response_text = match.group(0)\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Claude API Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datasets_with_claude(\n",
    "    paper_text: str, \n",
    "    paper_name: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    model: str = \"claude-sonnet-4-20250514\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Use Claude API to extract dataset information from paper text.\n",
    "    \n",
    "    Args:\n",
    "        paper_text: The extracted text from the paper\n",
    "        paper_name: Name of the paper (for logging and output)\n",
    "        api_key: Anthropic API key (uses env var if not provided)\n",
    "        model: Claude model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing dataset information\n",
    "    \"\"\"\n",
    "    logger.info(f\"Sending paper to Claude for analysis: {paper_name}\")\n",
    "    \n",
    "    # Initialize client\n",
    "    try:\n",
    "        client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize Anthropic client: {e}\")\n",
    "        logger.error(\"Make sure ANTHROPIC_API_KEY is set in your environment or .env file\")\n",
    "        raise\n",
    "    \n",
    "    # Truncate text if too long (keeping first and last parts for context)\n",
    "    max_chars = 150000  # Leave room for prompt and response\n",
    "    if len(paper_text) > max_chars:\n",
    "        half = max_chars // 2\n",
    "        paper_text = paper_text[:half] + \"\\n\\n[... middle section truncated ...]\\n\\n\" + paper_text[-half:]\n",
    "        logger.warning(f\"Paper text truncated to {max_chars} characters\")\n",
    "    \n",
    "    prompt = EXTRACTION_PROMPT.format(paper_text=paper_text)\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = message.content[0].text\n",
    "        \n",
    "        # Log the raw response for debugging (first 500 chars)\n",
    "        logger.debug(f\"Raw Claude response (first 500 chars): {response_text[:500]}\")\n",
    "        \n",
    "        # Clean up the response\n",
    "        cleaned_response = clean_json_response(response_text)\n",
    "        \n",
    "        logger.debug(f\"Cleaned response (first 500 chars): {cleaned_response[:500]}\")\n",
    "        \n",
    "        # Parse JSON\n",
    "        datasets = json.loads(cleaned_response)\n",
    "        \n",
    "        # Validate it's a list\n",
    "        if not isinstance(datasets, list):\n",
    "            logger.error(f\"Expected JSON array, got {type(datasets)}\")\n",
    "            logger.error(f\"Response: {cleaned_response[:1000]}\")\n",
    "            return []\n",
    "        \n",
    "        # Add paper name to each dataset\n",
    "        for dataset in datasets:\n",
    "            if isinstance(dataset, dict):\n",
    "                dataset[\"paper\"] = paper_name\n",
    "            else:\n",
    "                logger.warning(f\"Unexpected dataset format: {dataset}\")\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(datasets)} datasets from {paper_name}\")\n",
    "        return datasets\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse Claude response as JSON: {e}\")\n",
    "        logger.error(f\"Problematic response (first 1000 chars): {cleaned_response[:1000]}\")\n",
    "        \n",
    "        # Save the problematic response to a file for inspection\n",
    "        error_file = f\"error_response_{paper_name}.txt\"\n",
    "        try:\n",
    "            with open(error_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Original response:\\n{response_text}\\n\\n\")\n",
    "                f.write(f\"Cleaned response:\\n{cleaned_response}\\n\\n\")\n",
    "                f.write(f\"Error: {e}\\n\")\n",
    "            logger.error(f\"Full response saved to {error_file} for debugging\")\n",
    "        except Exception as write_error:\n",
    "            logger.error(f\"Could not save error file: {write_error}\")\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling Claude API: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"✓ Claude extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers_folder(\n",
    "    input_folder: str,\n",
    "    output_file: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    model: str = \"claude-sonnet-4-20250514\",\n",
    "    max_pages: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all PDF papers in a folder and extract dataset information.\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing PDF papers\n",
    "        output_file: Path for output CSV file\n",
    "        api_key: Anthropic API key\n",
    "        model: Claude model to use\n",
    "        max_pages: Maximum pages to process per PDF\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all extracted datasets\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {input_folder}\")\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = list(input_path.glob(\"*.pdf\")) + list(input_path.glob(\"*.PDF\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {input_folder}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    all_datasets = []\n",
    "    successful_papers = 0\n",
    "    failed_papers = 0\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"Processing: {pdf_file.name}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # Extract text from PDF\n",
    "            paper_text = extract_text_from_pdf(str(pdf_file), max_pages)\n",
    "            \n",
    "            # Get paper name (filename without extension)\n",
    "            paper_name = pdf_file.stem\n",
    "            \n",
    "            # Extract datasets using Claude\n",
    "            datasets = extract_datasets_with_claude(\n",
    "                paper_text=paper_text,\n",
    "                paper_name=paper_name,\n",
    "                api_key=api_key,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            if datasets:\n",
    "                all_datasets.extend(datasets)\n",
    "                successful_papers += 1\n",
    "            else:\n",
    "                logger.warning(f\"No datasets extracted from {pdf_file.name}\")\n",
    "                failed_papers += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {pdf_file.name}: {e}\")\n",
    "            failed_papers += 1\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"\\n{'='*60}\")\n",
    "    logger.info(f\"Processing Summary:\")\n",
    "    logger.info(f\"  Successfully processed: {successful_papers} papers\")\n",
    "    logger.info(f\"  Failed: {failed_papers} papers\")\n",
    "    logger.info(f\"  Total datasets extracted: {len(all_datasets)}\")\n",
    "    logger.info(f\"{'='*60}\\n\")\n",
    "    \n",
    "    if not all_datasets:\n",
    "        logger.warning(\"No datasets extracted from any papers\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with consistent column order\n",
    "    columns = [\n",
    "        \"paper\",\n",
    "        \"dataset_name\", \n",
    "        \"dataset_type\",\n",
    "        \"official_path\",\n",
    "        \"events_total\",\n",
    "        \"events_used\",\n",
    "        \"collision_energy_tev\",\n",
    "        \"generator\",\n",
    "        \"doi\",\n",
    "        \"size_bytes\",\n",
    "        \"notes\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(all_datasets)\n",
    "    \n",
    "    # Ensure all columns exist\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"N/A\"\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "    logger.info(f\"Saved {len(df)} datasets to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Main processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Check API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for API key\n",
    "api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"⚠️  WARNING: No API key found!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Please set your ANTHROPIC_API_KEY in one of these ways:\")\n",
    "    print(\"1. Create a .env file with: ANTHROPIC_API_KEY=your-key-here\")\n",
    "    print(\"2. Run this in a cell: os.environ['ANTHROPIC_API_KEY'] = 'your-key-here'\")\n",
    "    print(\"3. Set it as a system environment variable\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"✓ API key found\")\n",
    "    print(f\"   Key starts with: {api_key[:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run the Extraction\n",
    "\n",
    "Execute this cell to start processing all PDFs in your folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STARTING DATASET EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input folder: {CONFIG['input_folder']}\")\n",
    "print(f\"Output file: {CONFIG['output_file']}\")\n",
    "print(f\"Model: {CONFIG['model']}\")\n",
    "print(f\"Max pages per PDF: {CONFIG['max_pages'] or 'All'}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    df = process_papers_folder(\n",
    "        input_folder=CONFIG[\"input_folder\"],\n",
    "        output_file=CONFIG[\"output_file\"],\n",
    "        api_key=api_key,\n",
    "        model=CONFIG[\"model\"],\n",
    "        max_pages=CONFIG[\"max_pages\"]\n",
    "    )\n",
    "    \n",
    "    if not df.empty:\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"✅ EXTRACTION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total datasets extracted: {len(df)}\")\n",
    "        print(f\"Papers processed: {df['paper'].nunique()}\")\n",
    "        print(f\"Output saved to: {CONFIG['output_file']}\")\n",
    "        print()\n",
    "        print(\"Dataset types found:\")\n",
    "        print(df['dataset_type'].value_counts().to_string())\n",
    "        print()\n",
    "        print(\"Collision energies:\")\n",
    "        print(df['collision_energy_tev'].value_counts().to_string())\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"⚠️  WARNING: No datasets were extracted.\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Check the log messages above for errors.\")\n",
    "        df = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"❌ EXTRACTION FAILED: {e}\")\n",
    "    print(\"=\" * 60)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Results\n",
    "\n",
    "View the extracted datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    print(f\"\\nShowing first 10 rows of {len(df)} total datasets:\\n\")\n",
    "    display(df.head(10))\n",
    "    \n",
    "    print(f\"\\n\\nDataFrame info:\")\n",
    "    df.info()\n",
    "else:\n",
    "    print(\"No data to display. Please check the extraction step above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Filter or Analyze Results\n",
    "\n",
    "You can further analyze the extracted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    # Example: Filter for Real Data only\n",
    "    real_data = df[df['dataset_type'] == 'Real Data']\n",
    "    print(f\"Real Data datasets: {len(real_data)}\")\n",
    "    \n",
    "    # Example: Filter for Simulated MC\n",
    "    simulated = df[df['dataset_type'] == 'Simulated MC']\n",
    "    print(f\"Simulated MC datasets: {len(simulated)}\")\n",
    "    \n",
    "    # Example: Group by paper\n",
    "    print(\"\\nDatasets per paper:\")\n",
    "    print(df.groupby('paper').size().sort_values(ascending=False))\n",
    "    \n",
    "    # Example: Show datasets with DOIs\n",
    "    with_doi = df[df['doi'].notna() & (df['doi'] != 'N/A') & (df['doi'] != 'null')]\n",
    "    print(f\"\\nDatasets with DOIs: {len(with_doi)}\")\n",
    "else:\n",
    "    print(\"No data to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optional: Export to Excel\n",
    "\n",
    "If you prefer Excel format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty:\n",
    "    excel_file = CONFIG['output_file'].replace('.csv', '.xlsx')\n",
    "    df.to_excel(excel_file, index=False, engine='openpyxl')\n",
    "    print(f\"✓ Also saved to Excel: {excel_file}\")\n",
    "else:\n",
    "    print(\"No data to export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
