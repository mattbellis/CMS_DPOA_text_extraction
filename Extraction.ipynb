{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfea8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from pdfs and process with GPT\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from single pdf file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_text(text, max_words=1500):\n",
    "    \"\"\"Split text into chunks of max words 1500\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = ' '.join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_with_gpt(prompt, model=\"gpt-5\"):\n",
    "    \"\"\"Send prompt to GPT. Defensive wrapper that returns text or error string.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        # Defensive access in case SDK shape differs\n",
    "        try:\n",
    "            return response.choices[0].message.content\n",
    "        except Exception:\n",
    "            try:\n",
    "                return response.output_text\n",
    "            except Exception:\n",
    "                return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "\n",
    "def extract_data_from_pdf_text(pdf_text, prompt):\n",
    "    \"\"\"Extract data for each text chunk\"\"\"\n",
    "    if \"{text_chunk}\" in prompt:\n",
    "        full_prompt = prompt.replace(\"{text_chunk}\", pdf_text)\n",
    "    else:\n",
    "        full_prompt = f\"{prompt}\\n\\nDocument:\\n{pdf_text}\"\n",
    "\n",
    "    return process_with_gpt(full_prompt)\n",
    "\n",
    "\n",
    "def parse_csv_response(response_text):\n",
    "    \"\"\"Parse GPT CSV responses into a dict; tolerant to units and formats.\n",
    "\n",
    "    Returns dict with keys: Title, Authors, Date, Dataset_Name,\n",
    "    Size_Events, Size_Files, Size_Bytes, Data_Format, Dataset_DOI\n",
    "    or None if parsing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not response_text:\n",
    "            return None\n",
    "        # remove markdown fences and common wrappers\n",
    "        response_text = response_text.replace('```csv', '').replace('```', '').strip()\n",
    "        lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "        if len(lines) < 2:\n",
    "            return None\n",
    "        reader = csv.DictReader(lines)\n",
    "        row = next(reader)\n",
    "\n",
    "        def textual_to_int(value):\n",
    "            if value is None:\n",
    "                return None\n",
    "            s = str(value).strip()\n",
    "            if not s or s.lower() in ['null', 'unknown', 'n/a', 'na', '']:\n",
    "                return None\n",
    "            # scientific notation\n",
    "            try:\n",
    "                if 'e' in s.lower():\n",
    "                    return int(float(s))\n",
    "            except Exception:\n",
    "                pass\n",
    "            s_clean = s.replace(',', '').lower()\n",
    "            m = re.search(r'([\\d\\.]+)\\s*(million|billion|thousand|m|b|k)', s_clean)\n",
    "            if m:\n",
    "                num = float(m.group(1))\n",
    "                scale = m.group(2)\n",
    "                if 'million' in scale or scale == 'm':\n",
    "                    return int(num * 1e6)\n",
    "                if 'billion' in scale or scale == 'b':\n",
    "                    return int(num * 1e9)\n",
    "                if 'thousand' in scale or scale == 'k':\n",
    "                    return int(num * 1e3)\n",
    "            digits = ''.join(c for c in s if c.isdigit())\n",
    "            if digits:\n",
    "                return int(digits)\n",
    "            return None\n",
    "\n",
    "        result = {}\n",
    "        result['Title'] = (row.get('Title') or row.get('title') or '').strip() or None\n",
    "        result['Authors'] = (row.get('Authors') or row.get('authors') or '').strip() or None\n",
    "        result['Date'] = (row.get('Date') or row.get('date') or '').strip() or None\n",
    "        result['Dataset_Name'] = (row.get('Dataset name (collision or MC)') or row.get('Dataset_Name') or row.get('Dataset name') or '').strip() or None\n",
    "\n",
    "        result['Size_Events'] = textual_to_int(row.get('Size (events)') or row.get('Size_Events') or '')\n",
    "        result['Size_Files'] = textual_to_int(row.get('Size (files)') or row.get('Size_Files') or '')\n",
    "        result['Size_Bytes'] = textual_to_int(row.get('Size (bytes)') or row.get('Size_Bytes') or '')\n",
    "\n",
    "        result['Data_Format'] = (row.get('Data format') or row.get('Data_Format') or '').strip() or None\n",
    "        result['Dataset_DOI'] = (row.get('Dataset DOI') or row.get('Dataset_DOI') or '').strip() or None\n",
    "\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing CSV response: {e}\")\n",
    "        print(response_text[:400])\n",
    "        return None\n",
    "\n",
    "\n",
    "def parse_json_like_response(text):\n",
    "    \"\"\"Find and parse a JSON object inside the model output.\"\"\"\n",
    "    try:\n",
    "        if not text:\n",
    "            return None\n",
    "        cleaned = text.replace('```', '')\n",
    "        m = re.search(r\"\\{[\\s\\S]*\\}\", cleaned)\n",
    "        if not m:\n",
    "            return None\n",
    "        json_text = m.group(0)\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_json_like_response error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_doi(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r'(10\\.\\d{4,9}/[^\\s,\\)\\]]+)', text)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "\n",
    "def find_date(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    m = re.search(r'(20\\d{2}-\\d{2})', text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m2 = re.search(r'(20\\d{2})', text)\n",
    "    return m2.group(1) if m2 else None\n",
    "\n",
    "\n",
    "def guess_sizes_from_text(text):\n",
    "    out = {'Size_Events': None, 'Size_Files': None, 'Size_Bytes': None}\n",
    "    if not text:\n",
    "        return out\n",
    "    # Search for explicit patterns\n",
    "    # e.g. '1.2 million events', '1,200,000 events', 'approx 1.2e6 events'\n",
    "    for m in re.finditer(r'([\\d,\\.]+)\\s*(million|billion|thousand|m|b|k)?\\s*(events?|files?|bytes?)', text, flags=re.I):\n",
    "        num_s = m.group(1).replace(',', '')\n",
    "        unit = (m.group(2) or '').lower()\n",
    "        target = m.group(3).lower()\n",
    "        try:\n",
    "            val = float(num_s)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if unit in ['million','m']:\n",
    "            val = int(val * 1e6)\n",
    "        elif unit in ['billion','b']:\n",
    "            val = int(val * 1e9)\n",
    "        elif unit in ['thousand','k']:\n",
    "            val = int(val * 1e3)\n",
    "        else:\n",
    "            val = int(val)\n",
    "        if 'event' in target and out['Size_Events'] is None:\n",
    "            out['Size_Events'] = int(val)\n",
    "        if 'file' in target and out['Size_Files'] is None:\n",
    "            out['Size_Files'] = int(val)\n",
    "        if 'byte' in target and out['Size_Bytes'] is None:\n",
    "            out['Size_Bytes'] = int(val)\n",
    "    return out\n",
    "\n",
    "\n",
    "def fill_missing_fields(parsed, llm_text, full_text):\n",
    "    \"\"\"Attempt to fill missing fields using llm_text (the model output) and full_text (PDF text).\n",
    "\n",
    "    Steps:\n",
    "      - search for DOI and date\n",
    "      - heuristic scan for sizes\n",
    "      - small LLM fallback returning JSON if many fields remain\n",
    "    \"\"\"\n",
    "    if parsed is None:\n",
    "        parsed = {}\n",
    "    combined = ((llm_text or '') + '\\n' + (full_text or ''))\n",
    "\n",
    "    # DOI\n",
    "    if not parsed.get('Dataset_DOI'):\n",
    "        doi = find_doi(combined)\n",
    "        if doi:\n",
    "            parsed['Dataset_DOI'] = doi\n",
    "\n",
    "    # Date\n",
    "    if not parsed.get('Date'):\n",
    "        d = find_date(combined)\n",
    "        if d:\n",
    "            parsed['Date'] = d\n",
    "\n",
    "    # sizes\n",
    "    sizes = guess_sizes_from_text(combined)\n",
    "    for k in ['Size_Events','Size_Files','Size_Bytes']:\n",
    "        if not parsed.get(k) and sizes.get(k) is not None:\n",
    "            parsed[k] = sizes[k]\n",
    "\n",
    "    # dataset name\n",
    "    if not parsed.get('Dataset_Name'):\n",
    "        m = re.search(r'(/[^\\n,]+/[^\\n,]+/[^\\n,]+/AOD|/[^\\n,]+/[^\\n,]+/AODSIM)', combined)\n",
    "        if m:\n",
    "            parsed['Dataset_Name'] = m.group(0)\n",
    "\n",
    "    # if still missing many fields, do an LLM JSON fallback\n",
    "    needed = [k for k in ['Title','Authors','Date','Dataset_Name','Size_Events','Size_Files','Size_Bytes','Data_Format','Dataset_DOI'] if not parsed.get(k)]\n",
    "    if needed:\n",
    "        fallback_prompt = (\n",
    "            \"Given the following document text, return a compact JSON object with these keys: \"\n",
    "            \"Title, Authors, Date, Dataset_Name, Size_Events, Size_Files, Size_Bytes, Data_Format, Dataset_DOI. \"\n",
    "            \"Use null for unknown values. Numbers must be integers (no commas). Return ONLY valid JSON.\\n\\n\" + (full_text or '')[:3000]\n",
    "        )\n",
    "        try:\n",
    "            fb_resp = process_with_gpt(fallback_prompt)\n",
    "            fb_json = parse_json_like_response(fb_resp)\n",
    "            if fb_json:\n",
    "                for k,v in fb_json.items():\n",
    "                    if not parsed.get(k) and v is not None:\n",
    "                        if k in ['Size_Events','Size_Files','Size_Bytes']:\n",
    "                            try:\n",
    "                                parsed[k] = int(v)\n",
    "                            except Exception:\n",
    "                                parsed[k] = parsed.get(k) or None\n",
    "                        else:\n",
    "                            parsed[k] = v\n",
    "        except Exception as e:\n",
    "            print(f\"LLM fallback error: {e}\")\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory, prompt, output_csv=\"extracted_results.csv\", chunk_threshold=1500):\n",
    "    \"\"\"Process the pdfs and output to CSV\"\"\"\n",
    "\n",
    "    results = []\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"Processing {i}/{len(pdf_files)}: {pdf_path.name}\")\n",
    "        try:\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            if not pdf_text:\n",
    "                print(f\" - No text extracted from {pdf_path.name}\")\n",
    "                continue\n",
    "            word_count = len(pdf_text.split())\n",
    "            print(f\"  Extracted {word_count} words\")\n",
    "\n",
    "            if word_count > chunk_threshold:\n",
    "                print(f\"  Chunking document (>{chunk_threshold} words)\")\n",
    "                chunks = chunk_text(pdf_text, max_words=chunk_threshold)\n",
    "                chunk_outputs = []\n",
    "                for idx, chunk in enumerate(chunks, 1):\n",
    "                    print(f\"    Processing chunk {idx}/{len(chunks)}\")\n",
    "                    if \"{text_chunk}\" in prompt:\n",
    "                        chunk_prompt = prompt.replace(\"{text_chunk}\", chunk)\n",
    "                    else:\n",
    "                        chunk_prompt = f\"{prompt}\\n\\nDocument chunk:\\n{chunk}\"\n",
    "                    out = process_with_gpt(chunk_prompt)\n",
    "                    chunk_outputs.append(out)\n",
    "                combined_prompt = (\n",
    "                    \"Combine the following chunked extractions into a single \"\n",
    "                    \"coherent CSV row. Remove duplicates and consolidate the data. Return ONLY the CSV with header and one data row:\\n\\n\" + \"\\n\\n---\\n\\n\".join(chunk_outputs)\n",
    "                )\n",
    "                final = process_with_gpt(combined_prompt)\n",
    "            else:\n",
    "                final = extract_data_from_pdf_text(pdf_text, prompt)\n",
    "\n",
    "            parsed_data = parse_csv_response(final)\n",
    "            if parsed_data:\n",
    "                parsed_data = fill_missing_fields(parsed_data, final, pdf_text)\n",
    "                parsed_data['Source_File'] = pdf_path.name\n",
    "                results.append(parsed_data)\n",
    "                print(f\"✓ Successfully processed {pdf_path.name}\")\n",
    "            else:\n",
    "                print(f\"⚠ Could not parse response for {pdf_path.name}, trying fallback\")\n",
    "                fallback_prompt = (\n",
    "                    \"You previously attempted to produce a CSV row but parsing failed. \"\n",
    "                    \"Given the following document text, return a JSON object with keys: \"\n",
    "                    \"Title, Authors, Date, Dataset_Name, Size_Events, Size_Files, Size_Bytes, Data_Format, Dataset_DOI. \"\n",
    "                    \"Use null for unknown values, numbers as plain integers (no commas), and try to approximate missing sizes. \"\n",
    "                    \"Return ONLY valid JSON.\\n\\n\" + pdf_text[:2000]\n",
    "                )\n",
    "                fallback_resp = process_with_gpt(fallback_prompt)\n",
    "                parsed_data = parse_json_like_response(fallback_resp)\n",
    "                if parsed_data:\n",
    "                    parsed_data = fill_missing_fields(parsed_data, fallback_resp, pdf_text)\n",
    "                    parsed_data['Source_File'] = pdf_path.name\n",
    "                    results.append(parsed_data)\n",
    "                    print(f\"✓ Successfully processed {pdf_path.name} (fallback)\")\n",
    "                else:\n",
    "                    print(f\"✗ Fallback also failed for {pdf_path.name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {pdf_path.name}: {str(e)}\")\n",
    "\n",
    "    if results and output_csv:\n",
    "        df = pd.DataFrame(results)\n",
    "        column_order = ['Source_File', 'Title', 'Authors', 'Date', 'Dataset_Name', 'Size_Events', 'Size_Files', 'Size_Bytes', 'Data_Format', 'Dataset_DOI']\n",
    "        df = df[[col for col in column_order if col in df.columns]]\n",
    "        df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "        print(f\"\\n✓ Results saved to {output_csv}\")\n",
    "        print(f\"✓ Processed {len(results)} PDFs successfully\")\n",
    "        print(\"\\nData Summary:\")\n",
    "        print(df[['Source_File', 'Title', 'Size_Events', 'Size_Files', 'Size_Bytes']].to_string())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage (kept for convenience; you can call process_multiple_pdfs elsewhere)\n",
    "if __name__ == \"__main__\":\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \\\"events\\\" and datasets. You are also very, very careful and a good explainer.\n",
    "    I need your help reading documents and extracting information about datasets used.\n",
    "\n",
    "    Extract the following information:\n",
    "    * Title of the paper\n",
    "    * Authors of the paper (comma-separated if multiple)\n",
    "    * Date of publication in YYYY-MM format (e.g., 2023-06)\n",
    "    * Dataset name (collision or MC)\n",
    "    * Size in number of events (as a plain integer, no commas)\n",
    "    * Size in number of files (as a plain integer, no commas)\n",
    "    * Size in bytes (as a plain integer, no commas or units)\n",
    "    * Data format (AOD, miniAOD, nanoAOD, etc)\n",
    "    * DOI of datasets used\n",
    "\n",
    "    If exact numbers are not given, provide approximations without noting them as such.\n",
    "    If you can't find data in the paper, look it up from cited CMS Open Data records or DOIs.\n",
    "    Use regular hyphens (-) not em dashes (—).\n",
    "\n",
    "    Return ONLY a CSV with this exact header:\n",
    "    Title,Authors,Date,Dataset name (collision or MC),Size (events),Size (files),Size (bytes),Data format,Dataset DOI\n",
    "\n",
    "    Then one data row with the extracted values. Use \\\"null\\\" for unknown values.\n",
    "    NO explanations, NO markdown formatting, JUST the CSV.\n",
    "\n",
    "    Document Text:\n",
    "    \"{text_chunk}\"\n",
    "    \"\"\"\n",
    "\n",
    "    results = process_multiple_pdfs(\n",
    "        pdf_directory=r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        output_csv=\"extracted_results.csv\",\n",
    "        chunk_threshold=1500\n",
    "    )\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total papers processed: {len(results)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
