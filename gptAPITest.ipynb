{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aefbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with orginial paper- jet substructure\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"jet_substructure_paper.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell ONCE\n",
    "!curl https://arxiv.org/pdf/2312.06909v1 -o pretraining_strat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12035234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying other paper\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"pretraining_strat.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample csv file from the output from pretraining paper and jet substructure paper (find a way to automate just using chat response instead of manually copying)\n",
    "data = [\n",
    "         [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleElectron primary dataset Run2015D-08Jun2016-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24103\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleMuon primary dataset Run2015D-16Dec2015-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24102\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Private MC: 2HDM signal plus SM ttbar background (MC)\",\"~1200000 (total across train, val, test)\",\"N/A\",\"N/A\",\"Delphes fast-sim ROOT files\",\"N/A\"],\n",
    "         [\"Jet Substructure Studies with CMS Open Data\",\"Aashish Tripathee; Wei Xue; Andrew Larkoski; Simone Marzani; Jesse Thaler\",\"CMS Open Data - Jet Primary Dataset (/Jet/Run2010B-Apr21ReReco-v1/AOD), pp collision data at 7 TeV\",\"20022826\",\"1664\",\"2000000000000\",\"AOD\",\"10.7483/OPENDATA.CMS.3S7F.2E9W\"],\n",
    "\n",
    "        \n",
    "     ]\n",
    "\n",
    "with open('output.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path): #extract text from single pdf file \n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=1500): #Split text into chunks of max words 1500\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = ' '.join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_with_gpt(prompt, model=\"gpt-5\"): #send prompt to GPT\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    \n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "def extract_data_from_pdf_text(pdf_text, prompt): #extract data for each text chunk\n",
    "    if \"{text_chunk}\" in prompt:\n",
    "        full_prompt = prompt.replace(\"{text_chunk}\", pdf_text)\n",
    "    else:\n",
    "        full_prompt = f\"{prompt}\\n\\nDocument:\\n{pdf_text}\"\n",
    "    \n",
    "    return process_with_gpt(full_prompt)\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory, prompt, output_file=None, chunk_threshold=1500): #process the pdfs\n",
    "    \n",
    "    results = {}\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"Processing {i}/{len(pdf_files)}: {pdf_path.name}\")\n",
    "        try:\n",
    "            # Extract text from current PDF only\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            if not pdf_text:\n",
    "                print(f\" - No text extracted from {pdf_path.name}\")\n",
    "                results[pdf_path.name] = \"No text found\"\n",
    "                continue\n",
    "\n",
    "            word_count = len(pdf_text.split())\n",
    "            print(f\"  Extracted {word_count} words\")\n",
    "\n",
    "            # Process with chunking if needed\n",
    "            if word_count > chunk_threshold:\n",
    "                print(f\"  Chunking document (>{chunk_threshold} words)\")\n",
    "                chunks = chunk_text(pdf_text, max_words=chunk_threshold)\n",
    "                chunk_outputs = []\n",
    "                \n",
    "                for idx, chunk in enumerate(chunks, 1):\n",
    "                    print(f\"    Processing chunk {idx}/{len(chunks)}\")\n",
    "                    if \"{text_chunk}\" in prompt:\n",
    "                        chunk_prompt = prompt.replace(\"{text_chunk}\", chunk)\n",
    "                    else:\n",
    "                        chunk_prompt = f\"{prompt}\\n\\nDocument chunk:\\n{chunk}\"\n",
    "                    \n",
    "                    out = process_with_gpt(chunk_prompt)\n",
    "                    chunk_outputs.append(out)\n",
    "                \n",
    "                # Combine chunk results\n",
    "                combined_prompt = (\n",
    "                    \"Combine the following chunked extractions into a single \"\n",
    "                    \"coherent extraction. Remove duplicates and consolidate \"\n",
    "                    \"the data:\\n\\n\" + \"\\n\\n---\\n\\n\".join(chunk_outputs)\n",
    "                )\n",
    "                final = process_with_gpt(combined_prompt)\n",
    "            else:\n",
    "                # Process entire document at once\n",
    "                final = extract_data_from_pdf_text(pdf_text, prompt)\n",
    "\n",
    "            results[pdf_path.name] = final\n",
    "            print(f\"✓ Successfully processed {pdf_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {pdf_path.name}: {str(e)}\")\n",
    "            results[pdf_path.name] = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Save results to file if specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for filename, data in results.items():\n",
    "                f.write(f\"\\n{'='*60}\\n\")\n",
    "                f.write(f\"File: {filename}\\n\")\n",
    "                f.write(f\"{'='*60}\\n\")\n",
    "                f.write(f\"{data}\\n\")\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your extraction prompt\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer.\n",
    "    I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like:\n",
    "    * Title of the paper\n",
    "    * Authors of the paper\n",
    "    * Name of the dataset (collision or MC)\n",
    "    * Size in number of events\n",
    "    * Size in number of files\n",
    "    * Size in bytes\n",
    "    * Dataformat (AOD, miniAOD, nanoAOD, etc)\n",
    "    * Doi of datasets used\n",
    "    If the paper does not give exact numbers, provide an approximation and tag it as an approximation.\n",
    "    Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "    Produce a single CSV-style row (or a CSV with one header row + one data row) for each document with columns: Title, Authors, Dataset name (collision or MC), Size (events), Size (files), Size (bytes), Data format, Dataset DOI.\n",
    "    If you are given a text chunk, insert it where {text_chunk} appears in this template. If not, the document text will be appended after this prompt for context.\n",
    "\n",
    "    Document Text:\n",
    "    \"{text_chunk}\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process PDFs\n",
    "    results = process_multiple_pdfs(\n",
    "        pdf_directory=r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        output_file=\"extracted_results.txt\",\n",
    "        chunk_threshold=1500\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    for filename in results:\n",
    "        print(f\"- {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3af2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv file with result data (automate this later)\n",
    "#Should be 8 items in the list\n",
    "#chat chatGPT doesn't output this in proper so have to fix this manually for now\n",
    "\n",
    "initial_data = [\n",
    "    [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"CMS Run 2012 DoubleMuParked dimuon sample (8 TeV, collision) - AOD; likely Runs 2012B/2012C/2012D\",\"~1e8-1e9 (approximation)\",\"~1e3-1.5e4 (approximation)\",\"~1e13-1e14 (approximation)\",\"AOD\",\"Lookup required (not specified in paper)\"],\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"Published spectra from BaBar; LHCb; NA48/2; APEX; HADES; KLOE; PHENIX; WASA; E774; E141; NA64 (phenomenology recast; no CMS Open Data)\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\"],\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"CMS 2012 8 TeV MET primary datasets (Run2012A, Run2012B, Run2012C) with HLT_PFMET150 (collision)\",\"approximate: B+C ~4.3e7 total; overall O(1e7-1e8)\",\"approximate: O(1e3-1e4)\",\"approximate: O(1-10 TB)\",\"AOD\",\"requires lookup - per-dataset DOIs for /MET/Run2012A-22Jan2013-v1/AOD; /MET/Run2012B-22Jan2013-v1/AOD; /MET/Run2012C-22Jan2013-v1/AOD\"],\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"TTJets_HadronicMGDecays_8TeV-madgraph (MC - full detector simulation)\",\"approximate: unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"requires lookup - CMS Open Data record\"],\n",
    "    [\"End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data\",\"M. Andrews; J. Alison; S. An; B. Burkle; S. Gleyzer; M. Narain; M. Paulini; B. Poczos; E. Usai\",\"CMS 2012 Open Data simulated QCD dijet (Pythia6 Z2*, pThat 80-120 and 120-170) (MC)\",\"~933,206 used (subset)\",\"not specified\",\"not specified\",\"GEN-SIM-RECO (RECO)\",\"not specified\"],\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleElectron/Run2015D-08Jun2016-v1/AOD\",\"approx-90,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-4,500\",\"approx-5e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24103\"],\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleMuon/Run2015D-16Dec2015-v1/AOD\",\"approx-120,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-6,000\",\"approx-7e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24102\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"/DoubleMu/Run2011A-12Oct2013-v1/AOD (collision)\",\"approx 2.0e7 total; 6,241,576 analyzed; 2,155,900 after baseline\",\"approx 2000\",\"approx 6.0e12\",\"AOD\",\"unknown - please verify on CERN Open Data\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYJetsToLL_M-50_TuneZ2_7TeV-madgraph-tauola (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYToMuMu_M-10To50_TuneZ2_7TeV-pythia6 (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"]\n",
    "]\n",
    "\n",
    "with open('outputTest.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b9464ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Dataset name (collision or MC)</th>\n",
       "      <th>Size (events)</th>\n",
       "      <th>Size (files)</th>\n",
       "      <th>Size (bytes)</th>\n",
       "      <th>Data format</th>\n",
       "      <th>Dataset DOI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unveiling Time-Varying Signals of Ultralight B...</td>\n",
       "      <td>Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang...</td>\n",
       "      <td>CMS Run 2012 DoubleMuParked dimuon sample (8 T...</td>\n",
       "      <td>~1e8-1e9 (approximation)</td>\n",
       "      <td>~1e3-1.5e4 (approximation)</td>\n",
       "      <td>~1e13-1e14 (approximation)</td>\n",
       "      <td>AOD</td>\n",
       "      <td>Lookup required (not specified in paper)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unveiling Time-Varying Signals of Ultralight B...</td>\n",
       "      <td>Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang...</td>\n",
       "      <td>Published spectra from BaBar; LHCb; NA48/2; AP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exploring Uncharted Soft Displaced Vertices in...</td>\n",
       "      <td>Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang</td>\n",
       "      <td>CMS 2012 8 TeV MET primary datasets (Run2012A,...</td>\n",
       "      <td>approximate: B+C ~4.3e7 total; overall O(1e7-1e8)</td>\n",
       "      <td>approximate: O(1e3-1e4)</td>\n",
       "      <td>approximate: O(1-10 TB)</td>\n",
       "      <td>AOD</td>\n",
       "      <td>requires lookup - per-dataset DOIs for /MET/Ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploring Uncharted Soft Displaced Vertices in...</td>\n",
       "      <td>Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang</td>\n",
       "      <td>TTJets_HadronicMGDecays_8TeV-madgraph (MC - fu...</td>\n",
       "      <td>approximate: unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>AODSIM</td>\n",
       "      <td>requires lookup - CMS Open Data record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>End-to-End Jet Classification of Quarks and Gl...</td>\n",
       "      <td>M. Andrews; J. Alison; S. An; B. Burkle; S. Gl...</td>\n",
       "      <td>CMS 2012 Open Data simulated QCD dijet (Pythia...</td>\n",
       "      <td>~933,206 used (subset)</td>\n",
       "      <td>not specified</td>\n",
       "      <td>not specified</td>\n",
       "      <td>GEN-SIM-RECO (RECO)</td>\n",
       "      <td>not specified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pre-training strategy using real particle coll...</td>\n",
       "      <td>Tomoe Kishimoto; Masahiro Morinaga; Masahiko S...</td>\n",
       "      <td>collision - /SingleElectron/Run2015D-08Jun2016...</td>\n",
       "      <td>approx-90,000,000 (full dataset); subset used ...</td>\n",
       "      <td>approx-4,500</td>\n",
       "      <td>approx-5e12</td>\n",
       "      <td>AOD (source); MiniAOD used in analysis</td>\n",
       "      <td>http://opendata.cern.ch/record/24103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pre-training strategy using real particle coll...</td>\n",
       "      <td>Tomoe Kishimoto; Masahiro Morinaga; Masahiko S...</td>\n",
       "      <td>collision - /SingleMuon/Run2015D-16Dec2015-v1/AOD</td>\n",
       "      <td>approx-120,000,000 (full dataset); subset used...</td>\n",
       "      <td>approx-6,000</td>\n",
       "      <td>approx-7e12</td>\n",
       "      <td>AOD (source); MiniAOD used in analysis</td>\n",
       "      <td>http://opendata.cern.ch/record/24102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Searching in CMS Open Data for Dimuon Resonanc...</td>\n",
       "      <td>Cari Cesarotti; Yotam Soreq; Matthew J. Strass...</td>\n",
       "      <td>/DoubleMu/Run2011A-12Oct2013-v1/AOD (collision)</td>\n",
       "      <td>approx 2.0e7 total; 6,241,576 analyzed; 2,155,...</td>\n",
       "      <td>approx 2000</td>\n",
       "      <td>approx 6.0e12</td>\n",
       "      <td>AOD</td>\n",
       "      <td>unknown - please verify on CERN Open Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Searching in CMS Open Data for Dimuon Resonanc...</td>\n",
       "      <td>Cari Cesarotti; Yotam Soreq; Matthew J. Strass...</td>\n",
       "      <td>DYJetsToLL_M-50_TuneZ2_7TeV-madgraph-tauola (MC)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>AODSIM</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Searching in CMS Open Data for Dimuon Resonanc...</td>\n",
       "      <td>Cari Cesarotti; Yotam Soreq; Matthew J. Strass...</td>\n",
       "      <td>DYToMuMu_M-10To50_TuneZ2_7TeV-pythia6 (MC)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>AODSIM</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Unveiling Time-Varying Signals of Ultralight B...   \n",
       "1  Unveiling Time-Varying Signals of Ultralight B...   \n",
       "2  Exploring Uncharted Soft Displaced Vertices in...   \n",
       "3  Exploring Uncharted Soft Displaced Vertices in...   \n",
       "4  End-to-End Jet Classification of Quarks and Gl...   \n",
       "5  Pre-training strategy using real particle coll...   \n",
       "6  Pre-training strategy using real particle coll...   \n",
       "7  Searching in CMS Open Data for Dimuon Resonanc...   \n",
       "8  Searching in CMS Open Data for Dimuon Resonanc...   \n",
       "9  Searching in CMS Open Data for Dimuon Resonanc...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang...   \n",
       "1  Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang...   \n",
       "2         Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang   \n",
       "3         Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang   \n",
       "4  M. Andrews; J. Alison; S. An; B. Burkle; S. Gl...   \n",
       "5  Tomoe Kishimoto; Masahiro Morinaga; Masahiko S...   \n",
       "6  Tomoe Kishimoto; Masahiro Morinaga; Masahiko S...   \n",
       "7  Cari Cesarotti; Yotam Soreq; Matthew J. Strass...   \n",
       "8  Cari Cesarotti; Yotam Soreq; Matthew J. Strass...   \n",
       "9  Cari Cesarotti; Yotam Soreq; Matthew J. Strass...   \n",
       "\n",
       "                      Dataset name (collision or MC)  \\\n",
       "0  CMS Run 2012 DoubleMuParked dimuon sample (8 T...   \n",
       "1  Published spectra from BaBar; LHCb; NA48/2; AP...   \n",
       "2  CMS 2012 8 TeV MET primary datasets (Run2012A,...   \n",
       "3  TTJets_HadronicMGDecays_8TeV-madgraph (MC - fu...   \n",
       "4  CMS 2012 Open Data simulated QCD dijet (Pythia...   \n",
       "5  collision - /SingleElectron/Run2015D-08Jun2016...   \n",
       "6  collision - /SingleMuon/Run2015D-16Dec2015-v1/AOD   \n",
       "7    /DoubleMu/Run2011A-12Oct2013-v1/AOD (collision)   \n",
       "8   DYJetsToLL_M-50_TuneZ2_7TeV-madgraph-tauola (MC)   \n",
       "9         DYToMuMu_M-10To50_TuneZ2_7TeV-pythia6 (MC)   \n",
       "\n",
       "                                       Size (events)  \\\n",
       "0                           ~1e8-1e9 (approximation)   \n",
       "1                                                NaN   \n",
       "2  approximate: B+C ~4.3e7 total; overall O(1e7-1e8)   \n",
       "3                               approximate: unknown   \n",
       "4                             ~933,206 used (subset)   \n",
       "5  approx-90,000,000 (full dataset); subset used ...   \n",
       "6  approx-120,000,000 (full dataset); subset used...   \n",
       "7  approx 2.0e7 total; 6,241,576 analyzed; 2,155,...   \n",
       "8                                            unknown   \n",
       "9                                            unknown   \n",
       "\n",
       "                 Size (files)                Size (bytes)  \\\n",
       "0  ~1e3-1.5e4 (approximation)  ~1e13-1e14 (approximation)   \n",
       "1                         NaN                         NaN   \n",
       "2     approximate: O(1e3-1e4)     approximate: O(1-10 TB)   \n",
       "3                     unknown                     unknown   \n",
       "4               not specified               not specified   \n",
       "5                approx-4,500                 approx-5e12   \n",
       "6                approx-6,000                 approx-7e12   \n",
       "7                 approx 2000               approx 6.0e12   \n",
       "8                     unknown                     unknown   \n",
       "9                     unknown                     unknown   \n",
       "\n",
       "                              Data format  \\\n",
       "0                                     AOD   \n",
       "1                                     NaN   \n",
       "2                                     AOD   \n",
       "3                                  AODSIM   \n",
       "4                     GEN-SIM-RECO (RECO)   \n",
       "5  AOD (source); MiniAOD used in analysis   \n",
       "6  AOD (source); MiniAOD used in analysis   \n",
       "7                                     AOD   \n",
       "8                                  AODSIM   \n",
       "9                                  AODSIM   \n",
       "\n",
       "                                         Dataset DOI  \n",
       "0           Lookup required (not specified in paper)  \n",
       "1                                                NaN  \n",
       "2  requires lookup - per-dataset DOIs for /MET/Ru...  \n",
       "3             requires lookup - CMS Open Data record  \n",
       "4                                      not specified  \n",
       "5               http://opendata.cern.ch/record/24103  \n",
       "6               http://opendata.cern.ch/record/24102  \n",
       "7          unknown - please verify on CERN Open Data  \n",
       "8                                            unknown  \n",
       "9                                            unknown  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('outputTest.csv')\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
