{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with orginial paper- jet substructure\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"jet_substructure_paper.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell ONCE\n",
    "!curl https://arxiv.org/pdf/2312.06909v1 -o pretraining_strat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12035234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying other paper\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"pretraining_strat.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "myprompt += \"Also let me know how you found/calculated/inferred the information in the paper for each of the items.\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e4f98",
   "metadata": {},
   "source": [
    "****GPT Response when asking to clarify where the data is found/inferred****\n",
    "Here is what I could extract from the paper you provided and (where possible) infer carefully. I also indicate where the paper is silent and what would need to be looked up on the CMS Open Data portal to fill in the gaps.\n",
    "\n",
    "CSV\n",
    "title,authors,dataset_name,size_events,size_files,size_bytes,dataformat,dataset_doi\n",
    "Pre-training strategy using real particle collision data for event classification in collider physics,\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS Open Data - SingleElectron/Run2015D-08Jun2016-v1/AOD + SingleMuon/Run2015D-16Dec2015-v1/AOD (collision)\",approximately 1.2e6 events used after selection across both datasets (1.0e6 train, 0.1e6 val, 0.1e6 test),not stated in paper,not stated in paper,AOD,TBD (DOIs on CMS Open Data records 24103 and 24102)\n",
    "Pre-training strategy using real particle collision data for event classification in collider physics,\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Generated MC - Two-Higgs-Doublet-Model signal vs SM ttbar background with MadGraph5_aMC@NLO + Pythia8 + Delphes (MC)\",approximately 1.2e6 total across signal and background (1.0e6 train, 0.1e6 val, 0.1e6 test),not stated in paper,not stated in paper,Delphes ROOT (fast-sim),none\n",
    "\n",
    "How I found or inferred each item\n",
    "- Title of the paper: Front page of the PDF.\n",
    "- Authors of the paper: Front page of the PDF.\n",
    "- Name of the dataset (collision or MC):\n",
    "  - Collision data: Section 3 “Datasets,” Pre-training dataset, and references [12] and [13]. The paper explicitly uses the CMS open data SingleElectron and SingleMuon primary datasets from Run 2015D in AOD format.\n",
    "  - MC data: Section 3 “Event classification dataset” states events were generated with MadGraph5_aMC@NLO at sqrt(s)=13 TeV, showered with Pythia8, detector simulated by Delphes. Signal is a Two-Higgs-Doublet Model; background is SM ttbar.\n",
    "- Size in number of events:\n",
    "  - Collision data used in pre-training: Section 3 states selected events were split into approximately 10^6 for training, 10^5 for validation, and 10^5 for testing, i.e., about 1.2×10^6 events used in total across the two real-data datasets after their selections.\n",
    "  - MC data for event classification: Section 3 states approximately 5×10^5 train, 5×10^4 val, and 5×10^4 test events per process (signal and background). Summing across both processes gives about 1.0×10^6 train + 1.0×10^5 val + 1.0×10^5 test = 1.2×10^6 events total used.\n",
    "- Size in number of files: Not stated in the paper. This is typically listed on each CMS Open Data record page.\n",
    "- Size in bytes: Not stated in the paper. Also typically listed on the CMS Open Data record page.\n",
    "- Dataformat:\n",
    "  - Collision data: AOD, explicitly mentioned in refs [12] and [13] (“in AOD format”).\n",
    "  - MC: Delphes is explicitly mentioned; Delphes outputs are ROOT files (fast simulation).\n",
    "- DOI of datasets used:\n",
    "  - The paper cites the CMS Open Data record pages for SingleElectron Run2015D (record 24103) and SingleMuon Run2015D (record 24102) but does not print the DOI strings. Those record pages include the official DOIs (format 10.7483/OPENDATA.CMS....).\n",
    "  - The generated MC has no DOI.\n",
    "\n",
    "What I could not fill from the paper and how to complete it\n",
    "- Exact DOIs, number of files, and total bytes for the two CMS Run2015D AOD datasets are not printed in the paper. They can be retrieved from:\n",
    "  - SingleElectron Run2015D-08Jun2016-v1/AOD: http://opendata.cern.ch/record/24103\n",
    "  - SingleMuon   Run2015D-16Dec2015-v1/AOD: http://opendata.cern.ch/record/24102\n",
    "If you’d like, I can look up those records and add the exact DOIs and sizes to the CSV.\n",
    "\n",
    "Time to process: 119.66 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample csv file from the output from pretraining paper and jet substructure paper (find a way to automate just using chat response instead of manually copying)\n",
    "data = [\n",
    "         [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleElectron primary dataset Run2015D-08Jun2016-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24103\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleMuon primary dataset Run2015D-16Dec2015-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24102\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Private MC: 2HDM signal plus SM ttbar background (MC)\",\"~1200000 (total across train, val, test)\",\"N/A\",\"N/A\",\"Delphes fast-sim ROOT files\",\"N/A\"],\n",
    "         [\"Jet Substructure Studies with CMS Open Data\",\"Aashish Tripathee; Wei Xue; Andrew Larkoski; Simone Marzani; Jesse Thaler\",\"CMS Open Data - Jet Primary Dataset (/Jet/Run2010B-Apr21ReReco-v1/AOD), pp collision data at 7 TeV\",\"20022826\",\"1664\",\"2000000000000\",\"AOD\",\"10.7483/OPENDATA.CMS.3S7F.2E9W\"],\n",
    "\n",
    "        \n",
    "     ]\n",
    "\n",
    "with open('output.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path): #extract text from single pdf file \n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=1500): #Split text into chunks of max words 1500\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = ' '.join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_with_gpt(prompt, model=\"gpt-5\"): #send prompt to GPT\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    \n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "def extract_data_from_pdf_text(pdf_text, prompt): #extract data for each text chunk\n",
    "    if \"{text_chunk}\" in prompt:\n",
    "        full_prompt = prompt.replace(\"{text_chunk}\", pdf_text)\n",
    "    else:\n",
    "        full_prompt = f\"{prompt}\\n\\nDocument:\\n{pdf_text}\"\n",
    "    \n",
    "    return process_with_gpt(full_prompt)\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory, prompt, output_file=None, chunk_threshold=1500): #process the pdfs\n",
    "    \n",
    "    results = {}\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"Processing {i}/{len(pdf_files)}: {pdf_path.name}\")\n",
    "        try:\n",
    "            # Extract text from current PDF only\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            if not pdf_text:\n",
    "                print(f\" - No text extracted from {pdf_path.name}\")\n",
    "                results[pdf_path.name] = \"No text found\"\n",
    "                continue\n",
    "\n",
    "            word_count = len(pdf_text.split())\n",
    "            print(f\"  Extracted {word_count} words\")\n",
    "\n",
    "            # Process with chunking if needed\n",
    "            if word_count > chunk_threshold:\n",
    "                print(f\"  Chunking document (>{chunk_threshold} words)\")\n",
    "                chunks = chunk_text(pdf_text, max_words=chunk_threshold)\n",
    "                chunk_outputs = []\n",
    "                \n",
    "                for idx, chunk in enumerate(chunks, 1):\n",
    "                    print(f\"    Processing chunk {idx}/{len(chunks)}\")\n",
    "                    if \"{text_chunk}\" in prompt:\n",
    "                        chunk_prompt = prompt.replace(\"{text_chunk}\", chunk)\n",
    "                    else:\n",
    "                        chunk_prompt = f\"{prompt}\\n\\nDocument chunk:\\n{chunk}\"\n",
    "                    \n",
    "                    out = process_with_gpt(chunk_prompt)\n",
    "                    chunk_outputs.append(out)\n",
    "                \n",
    "                # Combine chunk results\n",
    "                combined_prompt = (\n",
    "                    \"Combine the following chunked extractions into a single \"\n",
    "                    \"coherent extraction. Remove duplicates and consolidate \"\n",
    "                    \"the data:\\n\\n\" + \"\\n\\n---\\n\\n\".join(chunk_outputs)\n",
    "                )\n",
    "                final = process_with_gpt(combined_prompt)\n",
    "            else:\n",
    "                # Process entire document at once\n",
    "                final = extract_data_from_pdf_text(pdf_text, prompt)\n",
    "\n",
    "            results[pdf_path.name] = final\n",
    "            print(f\"✓ Successfully processed {pdf_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {pdf_path.name}: {str(e)}\")\n",
    "            results[pdf_path.name] = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Save results to file if specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for filename, data in results.items():\n",
    "                f.write(f\"\\n{'='*60}\\n\")\n",
    "                f.write(f\"File: {filename}\\n\")\n",
    "                f.write(f\"{'='*60}\\n\")\n",
    "                f.write(f\"{data}\\n\")\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your extraction prompt\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer.\n",
    "    I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like:\n",
    "    * Title of the paper\n",
    "    * Authors of the paper\n",
    "    * Year and month of publication (make sure this is included in a format that will work as a float in a csv)\n",
    "    * Name of the dataset (collision or MC)\n",
    "    * Size in number of events\n",
    "    * The same size in number of events in a single number as a float for csv\n",
    "    * Size in number of files\n",
    "    * The same size in number of files in a single number as a float for csv\n",
    "    * Size in bytes\n",
    "    * The same size in bytes as a float in a single number for csv\n",
    "    * Dataformat (AOD, miniAOD, nanoAOD, etc)\n",
    "    * Doi of datasets used\n",
    "    If the paper does not give exact numbers, provide an approximation and tag it as an approximation.\n",
    "    Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "    Produce a single CSV-style row (or a CSV with one header row + one data row) for each document with columns: Title, Authors, Dataset name (collision or MC), Size (events), Size (files), Size (bytes), Data format, Dataset DOI.\n",
    "    If you are given a text chunk, insert it where {text_chunk} appears in this template. If not, the document text will be appended after this prompt for context.\n",
    "    \n",
    "\n",
    "    Document Text:\n",
    "    \"{text_chunk}\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process PDFs\n",
    "    results = process_multiple_pdfs(\n",
    "        pdf_directory=r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        output_file=\"extracted_results.txt\",\n",
    "        chunk_threshold=1500\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    for filename in results:\n",
    "        print(f\"- {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv file with result data (automate this later)\n",
    "#Should be 8 items in the list\n",
    "#chat chatGPT doesn't output this in proper so have to fix this manually for now\n",
    "\n",
    "initial_data = [\n",
    "    [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"CMS Run 2012 DoubleMuParked dimuon sample (8 TeV, collision) - AOD; likely Runs 2012B/2012C/2012D\",\"~1e8-1e9 (approximation)\",\"~1e3-1.5e4 (approximation)\",\"~1e13-1e14 (approximation)\",\"AOD\",\"Lookup required (not specified in paper)\"],\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"Published spectra from BaBar; LHCb; NA48/2; APEX; HADES; KLOE; PHENIX; WASA; E774; E141; NA64 (phenomenology recast; no CMS Open Data)\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\"],\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"CMS 2012 8 TeV MET primary datasets (Run2012A, Run2012B, Run2012C) with HLT_PFMET150 (collision)\",\"approximate: B+C ~4.3e7 total; overall O(1e7-1e8)\",\"approximate: O(1e3-1e4)\",\"approximate: O(1-10 TB)\",\"AOD\",\"requires lookup - per-dataset DOIs for /MET/Run2012A-22Jan2013-v1/AOD; /MET/Run2012B-22Jan2013-v1/AOD; /MET/Run2012C-22Jan2013-v1/AOD\"],\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"TTJets_HadronicMGDecays_8TeV-madgraph (MC - full detector simulation)\",\"approximate: unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"requires lookup - CMS Open Data record\"],\n",
    "    [\"End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data\",\"M. Andrews; J. Alison; S. An; B. Burkle; S. Gleyzer; M. Narain; M. Paulini; B. Poczos; E. Usai\",\"CMS 2012 Open Data simulated QCD dijet (Pythia6 Z2*, pThat 80-120 and 120-170) (MC)\",\"~933,206 used (subset)\",\"not specified\",\"not specified\",\"GEN-SIM-RECO (RECO)\",\"not specified\"],\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleElectron/Run2015D-08Jun2016-v1/AOD\",\"approx-90,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-4,500\",\"approx-5e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24103\"],\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleMuon/Run2015D-16Dec2015-v1/AOD\",\"approx-120,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-6,000\",\"approx-7e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24102\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"/DoubleMu/Run2011A-12Oct2013-v1/AOD (collision)\",\"approx 2.0e7 total; 6,241,576 analyzed; 2,155,900 after baseline\",\"approx 2000\",\"approx 6.0e12\",\"AOD\",\"unknown - please verify on CERN Open Data\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYJetsToLL_M-50_TuneZ2_7TeV-madgraph-tauola (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"],\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYToMuMu_M-10To50_TuneZ2_7TeV-pythia6 (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"]\n",
    "]\n",
    "\n",
    "with open('outputTest.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(initial_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9464ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('outputTest.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15f0686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('outputTest.h5', key='mainData')\n",
    "\n",
    "print(\"DataFrame saved to outputTest.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
