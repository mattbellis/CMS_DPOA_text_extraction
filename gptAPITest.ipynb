{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with orginial paper- jet substructure\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"jet_substructure_paper.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell ONCE\n",
    "!curl https://arxiv.org/pdf/2312.06909v1 -o pretraining_strat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12035234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying other paper\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"pretraining_strat.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample csv file from the output from pretraining paper and jet substructure paper (find a way to automate just using chat response instead of manually copying)\n",
    "data = [\n",
    "         [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleElectron primary dataset Run2015D-08Jun2016-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24103\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleMuon primary dataset Run2015D-16Dec2015-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24102\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Private MC: 2HDM signal plus SM ttbar background (MC)\",\"~1200000 (total across train, val, test)\",\"N/A\",\"N/A\",\"Delphes fast-sim ROOT files\",\"N/A\"],\n",
    "         [\"Jet Substructure Studies with CMS Open Data\",\"Aashish Tripathee; Wei Xue; Andrew Larkoski; Simone Marzani; Jesse Thaler\",\"CMS Open Data - Jet Primary Dataset (/Jet/Run2010B-Apr21ReReco-v1/AOD), pp collision data at 7 TeV\",\"20022826\",\"1664\",\"2000000000000\",\"AOD\",\"10.7483/OPENDATA.CMS.3S7F.2E9W\"],\n",
    "\n",
    "        \n",
    "     ]\n",
    "\n",
    "with open('output.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to extract multiple pdfs at once (not with RAG, just extract text from multiple pdfs in folder)\n",
    "#Probably too slow, so might just try to use RAG instead\n",
    "#Step 1: extract text from pdfs in folder\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder_path):\n",
    "    #extract text from all PDF files in folder\n",
    "    all_text = {} #holds the extracted text for each file\n",
    "    for filename in os.listdir(pdf_folder_path): #loop through files in folder\n",
    "        if not filename.lower().endswith('.pdf'): #only process pdf files\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename) #get path to pdf file\n",
    "\n",
    "        try:\n",
    "            doc = pymupdf.open(pdf_path)  # open by path \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        text_parts = [] #holds text parts for this file\n",
    "        try:\n",
    "            for page in doc:  # iterate pages\n",
    "                page_text = page.get_text() or \"\"\n",
    "                text_parts.append(page_text) #append the extracted text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "        finally: #ensure document is closed\n",
    "            try:\n",
    "                doc.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        all_text[filename] = \"\\n\".join(text_parts) #combine text parts\n",
    "\n",
    "    return all_text #return all_text\n",
    "\n",
    "# Define the path to folder containing the PDFs\n",
    "pdf_folder = r\"C:/Users/ejren/OneDrive/DPOA_papers\"\n",
    "# Run extraction\n",
    "document_texts = extract_text_from_pdfs(pdf_folder)\n",
    "print(f\"Extracted text from {len(document_texts)} PDF(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Chunk text for large pdfs so chatGPT doesn't lowkey crash out \n",
    "\n",
    "def chunk_text(text, chunk_size=1500, chunk_overlap=200): #chunk text into smaller pieces\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f432933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Generate prompt from chunks\n",
    "def generate_prompt(text_chunk):\n",
    "    #generate prompt for a given text chunk for GPT\n",
    "    return f\"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer. \n",
    "    I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like: \\n\n",
    "    * Title of the paper \\n\n",
    "    * Authors of the paper \\n\n",
    "    * Name of the dataset (collision or MC) \\n\n",
    "    * Size in number of events \\n\n",
    "    * Size in number of files \\n\n",
    "    * Size in bytes \\n\n",
    "    * Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\n",
    "    * Doi of datasets used \\n\n",
    "    I just uploaded to you a pdf of one of these papers. Can you try to extract that information? Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\n",
    "    Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "    Can you also create a csv file with that information, with columns for each of the items above?\n",
    "    \n",
    "    Document Chunk:\n",
    "    \"{text_chunk}\"\n",
    "\n",
    "    Summary:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a73410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Big boy chatGPT extraction (way over my head I have no idea what is going on I literally vibe coded this)\n",
    "\n",
    "def response_to_text(resp): #extract text from GPT response\n",
    "    \n",
    "    # Prefer a convenience property if present\n",
    "    if hasattr(resp, \"output_text\") and resp.output_text:\n",
    "        return resp.output_text\n",
    "\n",
    "    # Defensive extraction for common shapes\n",
    "    try:\n",
    "        out = getattr(resp, \"output\", None)\n",
    "        if out:\n",
    "            # out may be a list of objects with a 'content' field\n",
    "            if isinstance(out, (list, tuple)) and len(out) > 0:\n",
    "                first = out[0]\n",
    "                if isinstance(first, dict):\n",
    "                    # shape: {'content': [{'type':..., 'text': '...'}]}\n",
    "                    content = first.get(\"content\")\n",
    "                    if isinstance(content, (list, tuple)) and len(content) > 0:\n",
    "                        c0 = content[0]\n",
    "                        if isinstance(c0, dict) and \"text\" in c0:\n",
    "                            return c0[\"text\"]\n",
    "                        if isinstance(c0, dict) and \"content\" in c0 and isinstance(c0[\"content\"], str):\n",
    "                            return c0[\"content\"]\n",
    "                # fallback: string-cast the first element\n",
    "                return str(first)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Last resort: try dictionary conversion\n",
    "    try:\n",
    "        d = resp.to_dict()\n",
    "        return str(d)\n",
    "    except Exception:\n",
    "        return str(resp)\n",
    "\n",
    "\n",
    "def process_with_gpt(prompt_text, client=OpenAI(api_key=api_key), model=\"gpt-5\"):\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"OpenAI client is not configured (client is None)\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=prompt_text,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Re-raise with context so notebook shows a helpful message\n",
    "        print(f\"API call failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    text = response_to_text(resp)\n",
    "    # Ensure a string is returned\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_data(document_texts):\n",
    "    \"\"\"Process multiple documents: chunk, call model per chunk, and combine results.\"\"\"\n",
    "    results = {}\n",
    "    for filename, text in document_texts.items():\n",
    "        print(f\"Processing {filename}...\")\n",
    "        if not text or not text.strip():\n",
    "            print(\" - Empty document, skipping\")\n",
    "            results[filename] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Decide whether to chunk\n",
    "        if len(text.split()) > 1500:\n",
    "            chunks = chunk_text(text)\n",
    "            data_extracted_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_prompt = generate_prompt(chunk)\n",
    "                chunk_data_extract = process_with_gpt(chunk_prompt, client=OpenAI(api_key=api_key))\n",
    "                # Ensure chunk_data_extract is a string\n",
    "                if not isinstance(chunk_data_extract, str):\n",
    "                    chunk_data_extract = str(chunk_data_extract)\n",
    "                data_extracted_chunks.append(chunk_data_extract)\n",
    "                print(f\" - Data extracted chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "            # Combine chunk outputs and ask the model to consolidate into CSV-like output\n",
    "            combined_data_extraction_prompt = (\n",
    "                \"Combine the following data extractions into a single, cohesive CSV-format extraction:\\n\\n\"\n",
    "                + \"\\n\\n\".join(data_extracted_chunks)\n",
    "            )\n",
    "            final_data_extraction = process_with_gpt(combined_data_extraction_prompt, client=OpenAI(api_key=api_key))\n",
    "            results[filename] = final_data_extraction\n",
    "        else:\n",
    "            prompt = generate_prompt(text)\n",
    "            result_text = process_with_gpt(prompt, client=OpenAI(api_key=api_key))\n",
    "            if not isinstance(result_text, str):\n",
    "                result_text = str(result_text)\n",
    "            results[filename] = result_text\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# If running as a script, process and save outputs\n",
    "if __name__ == \"__main__\":\n",
    "    data_extractions = extract_data(document_texts)\n",
    "\n",
    "    # Print or save the results\n",
    "    for filename, data in data_extractions.items():\n",
    "        print(f\"\\n--- Summary for {filename} ---\\n{data}\\n\")\n",
    "        # Ensure we write a string\n",
    "        if data is None:\n",
    "            data = \"\"\n",
    "        if not isinstance(data, str):\n",
    "            data = str(data)\n",
    "        safe_name = filename.replace('.pdf', '').replace(' ', '_')\n",
    "        with open(f\"summary_{safe_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use RAG to extract data from multiple pdfs at once\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
