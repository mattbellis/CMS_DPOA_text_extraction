{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pymupdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import base64\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with orginial paper- jet substructure\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"jet_substructure_paper.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell ONCE\n",
    "!curl https://arxiv.org/pdf/2312.06909v1 -o pretraining_strat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12035234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying other paper\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"pretraining_strat.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample csv file from the output from pretraining paper and jet substructure paper (find a way to automate just using chat response instead of manually copying)\n",
    "data = [\n",
    "         [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleElectron primary dataset Run2015D-08Jun2016-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24103\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleMuon primary dataset Run2015D-16Dec2015-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24102\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Private MC: 2HDM signal plus SM ttbar background (MC)\",\"~1200000 (total across train, val, test)\",\"N/A\",\"N/A\",\"Delphes fast-sim ROOT files\",\"N/A\"],\n",
    "         [\"Jet Substructure Studies with CMS Open Data\",\"Aashish Tripathee; Wei Xue; Andrew Larkoski; Simone Marzani; Jesse Thaler\",\"CMS Open Data - Jet Primary Dataset (/Jet/Run2010B-Apr21ReReco-v1/AOD), pp collision data at 7 TeV\",\"20022826\",\"1664\",\"2000000000000\",\"AOD\",\"10.7483/OPENDATA.CMS.3S7F.2E9W\"],\n",
    "\n",
    "        \n",
    "     ]\n",
    "\n",
    "with open('output.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbecec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to extract multiple pdfs at once (not with RAG, just extract text from multiple pdfs in folder)\n",
    "#Probably too slow, so might just try to use RAG instead\n",
    "#Step 1: extract text from pdfs in folder\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder_path):\n",
    "    #extract text from all PDF files in folder\n",
    "    all_text = {} #holds the extracted text for each file\n",
    "    for filename in os.listdir(pdf_folder_path): #loop through files in folder\n",
    "        if not filename.lower().endswith('.pdf'): #only process pdf files\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename) #get path to pdf file\n",
    "\n",
    "        try:\n",
    "            doc = pymupdf.open(pdf_path)  # open by path \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to open {pdf_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        text_parts = [] #holds text parts for this file\n",
    "        try:\n",
    "            for page in doc:  # iterate pages\n",
    "                page_text = page.get_text() or \"\"\n",
    "                text_parts.append(page_text) #append the extracted text\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract text from {pdf_path}: {e}\")\n",
    "        finally: #ensure document is closed\n",
    "            try:\n",
    "                doc.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        all_text[filename] = \"\\n\".join(text_parts) #combine text parts\n",
    "\n",
    "    return all_text #return all_text\n",
    "\n",
    "# Define the path to folder containing the PDFs\n",
    "pdf_folder = r\"C:/Users/ejren/OneDrive/DPOA_papers\"\n",
    "# Run extraction\n",
    "document_texts = extract_text_from_pdfs(pdf_folder)\n",
    "print(f\"Extracted text from {len(document_texts)} PDF(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Chunk text for large pdfs so chatGPT doesn't lowkey crash out \n",
    "\n",
    "def chunk_text(text, chunk_size=1500, chunk_overlap=200): #chunk text into smaller pieces\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f432933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Generate prompt from chunks\n",
    "def generate_prompt(text_chunk):\n",
    "    #generate prompt for a given text chunk for GPT\n",
    "    return f\"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer. \n",
    "    I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like: \\n\n",
    "    * Title of the paper \\n\n",
    "    * Authors of the paper \\n\n",
    "    * Name of the dataset (collision or MC) \\n\n",
    "    * Size in number of events \\n\n",
    "    * Size in number of files \\n\n",
    "    * Size in bytes \\n\n",
    "    * Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\n",
    "    * Doi of datasets used \\n\n",
    "    I just uploaded to you a pdf of one of these papers. Can you try to extract that information? Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\n",
    "    Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "    Can you also create a csv file with that information, with columns for each of the items above?\n",
    "    \n",
    "    Document Chunk:\n",
    "    \"{text_chunk}\"\n",
    "\n",
    "    Summary:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a73410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Big boy chatGPT extraction (way over my head I have no idea what is going on I literally vibe coded this)\n",
    "\n",
    "def response_to_text(resp): #extract text from GPT response\n",
    "    \n",
    "    # Prefer a convenience property if present\n",
    "    if hasattr(resp, \"output_text\") and resp.output_text:\n",
    "        return resp.output_text\n",
    "\n",
    "    # Defensive extraction for common shapes\n",
    "    try:\n",
    "        out = getattr(resp, \"output\", None)\n",
    "        if out:\n",
    "            # out may be a list of objects with a 'content' field\n",
    "            if isinstance(out, (list, tuple)) and len(out) > 0:\n",
    "                first = out[0]\n",
    "                if isinstance(first, dict):\n",
    "                    # shape: {'content': [{'type':..., 'text': '...'}]}\n",
    "                    content = first.get(\"content\")\n",
    "                    if isinstance(content, (list, tuple)) and len(content) > 0:\n",
    "                        c0 = content[0]\n",
    "                        if isinstance(c0, dict) and \"text\" in c0:\n",
    "                            return c0[\"text\"]\n",
    "                        if isinstance(c0, dict) and \"content\" in c0 and isinstance(c0[\"content\"], str):\n",
    "                            return c0[\"content\"]\n",
    "                # fallback: string-cast the first element\n",
    "                return str(first)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Last resort: try dictionary conversion\n",
    "    try:\n",
    "        d = resp.to_dict()\n",
    "        return str(d)\n",
    "    except Exception:\n",
    "        return str(resp)\n",
    "\n",
    "\n",
    "def process_with_gpt(prompt_text, client=OpenAI(api_key=api_key), model=\"gpt-5\"):\n",
    "    if client is None:\n",
    "        raise RuntimeError(\"OpenAI client is not configured (client is None)\")\n",
    "\n",
    "    try:\n",
    "        \n",
    "        resp = client.responses.create(\n",
    "            model=model,\n",
    "            input=prompt_text,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Re-raise with context so notebook shows a helpful message\n",
    "        print(f\"API call failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    text = response_to_text(resp)\n",
    "    # Ensure a string is returned\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_data(document_texts):\n",
    "    \"\"\"Process multiple documents: chunk, call model per chunk, and combine results.\"\"\"\n",
    "    results = {}\n",
    "    for filename, text in document_texts.items():\n",
    "        print(f\"Processing {filename}...\")\n",
    "        if not text or not text.strip():\n",
    "            print(\" - Empty document, skipping\")\n",
    "            results[filename] = \"\"\n",
    "            continue\n",
    "\n",
    "        # Decide whether to chunk\n",
    "        if len(text.split()) > 1500:\n",
    "            chunks = chunk_text(text)\n",
    "            data_extracted_chunks = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_prompt = generate_prompt(chunk)\n",
    "                chunk_data_extract = process_with_gpt(chunk_prompt, client=OpenAI(api_key=api_key))\n",
    "                # Ensure chunk_data_extract is a string\n",
    "                if not isinstance(chunk_data_extract, str):\n",
    "                    chunk_data_extract = str(chunk_data_extract)\n",
    "                data_extracted_chunks.append(chunk_data_extract)\n",
    "                print(f\" - Data extracted chunk {i+1}/{len(chunks)}\")\n",
    "\n",
    "            # Combine chunk outputs and ask the model to consolidate into CSV-like output\n",
    "            combined_data_extraction_prompt = (\n",
    "                \"Combine the following data extractions into a single, cohesive CSV-format extraction:\\n\\n\"\n",
    "                + \"\\n\\n\".join(data_extracted_chunks)\n",
    "            )\n",
    "            final_data_extraction = process_with_gpt(combined_data_extraction_prompt, client=OpenAI(api_key=api_key))\n",
    "            results[filename] = final_data_extraction\n",
    "        else:\n",
    "            prompt = generate_prompt(text)\n",
    "            result_text = process_with_gpt(prompt, client=OpenAI(api_key=api_key))\n",
    "            if not isinstance(result_text, str):\n",
    "                result_text = str(result_text)\n",
    "            results[filename] = result_text\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# If running as a script, process and save outputs\n",
    "if __name__ == \"__main__\":\n",
    "    data_extractions = extract_data(document_texts)\n",
    "\n",
    "    # Print or save the results\n",
    "    for filename, data in data_extractions.items():\n",
    "        print(f\"\\n--- Summary for {filename} ---\\n{data}\\n\")\n",
    "        # Ensure we write a string\n",
    "        if data is None:\n",
    "            data = \"\"\n",
    "        if not isinstance(data, str):\n",
    "            data = str(data)\n",
    "        safe_name = filename.replace('.pdf', '').replace(' ', '_')\n",
    "        with open(f\"summary_{safe_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline prompt template to be used by processing helpers\n",
    "PROMPT_TEMPLATE = r\"\"\"\n",
    "You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer.\n",
    "I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like:\n",
    "* Title of the paper\n",
    "* Authors of the paper\n",
    "* Name of the dataset (collision or MC)\n",
    "* Size in number of events\n",
    "* Size in number of files\n",
    "* Size in bytes\n",
    "* Dataformat (AOD, miniAOD, nanoAOD, etc)\n",
    "* Doi of datasets used\n",
    "If the paper does not give exact numbers, provide an approximation and tag it as an approximation.\n",
    "Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "Produce a single CSV-style row (or a CSV with one header row + one data row) for each document with columns: Title, Authors, Dataset name (collision or MC), Size (events), Size (files), Size (bytes), Data format, Dataset DOI.\n",
    "If you are given a text chunk, insert it where {text_chunk} appears in this template. If not, the document text will be appended after this prompt for context.\n",
    "\n",
    "Document Chunk:\n",
    "\"{text_chunk}\"\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path): #extract text from single pdf file \n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, max_words=1500): #Split text into chunks of max words 1500\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = ' '.join(words[i:i + max_words])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def process_with_gpt(prompt, model=\"gpt-5\"): #send prompt to GPT\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    \n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"API Error: {str(e)}\"\n",
    "\n",
    "def extract_data_from_pdf_text(pdf_text, prompt): #extract data for each text chunk\n",
    "    if \"{text_chunk}\" in prompt:\n",
    "        full_prompt = prompt.replace(\"{text_chunk}\", pdf_text)\n",
    "    else:\n",
    "        full_prompt = f\"{prompt}\\n\\nDocument:\\n{pdf_text}\"\n",
    "    \n",
    "    return process_with_gpt(full_prompt)\n",
    "\n",
    "def process_multiple_pdfs(pdf_directory, prompt, output_file=None, chunk_threshold=1500): #process the pdfs\n",
    "    \n",
    "    results = {}\n",
    "    pdf_files = list(Path(pdf_directory).glob(\"*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {pdf_directory}\")\n",
    "\n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        print(f\"Processing {i}/{len(pdf_files)}: {pdf_path.name}\")\n",
    "        try:\n",
    "            # Extract text from current PDF only\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            \n",
    "            if not pdf_text:\n",
    "                print(f\" - No text extracted from {pdf_path.name}\")\n",
    "                results[pdf_path.name] = \"No text found\"\n",
    "                continue\n",
    "\n",
    "            word_count = len(pdf_text.split())\n",
    "            print(f\"  Extracted {word_count} words\")\n",
    "\n",
    "            # Process with chunking if needed\n",
    "            if word_count > chunk_threshold:\n",
    "                print(f\"  Chunking document (>{chunk_threshold} words)\")\n",
    "                chunks = chunk_text(pdf_text, max_words=chunk_threshold)\n",
    "                chunk_outputs = []\n",
    "                \n",
    "                for idx, chunk in enumerate(chunks, 1):\n",
    "                    print(f\"    Processing chunk {idx}/{len(chunks)}\")\n",
    "                    if \"{text_chunk}\" in prompt:\n",
    "                        chunk_prompt = prompt.replace(\"{text_chunk}\", chunk)\n",
    "                    else:\n",
    "                        chunk_prompt = f\"{prompt}\\n\\nDocument chunk:\\n{chunk}\"\n",
    "                    \n",
    "                    out = process_with_gpt(chunk_prompt)\n",
    "                    chunk_outputs.append(out)\n",
    "                \n",
    "                # Combine chunk results\n",
    "                combined_prompt = (\n",
    "                    \"Combine the following chunked extractions into a single \"\n",
    "                    \"coherent extraction. Remove duplicates and consolidate \"\n",
    "                    \"the data:\\n\\n\" + \"\\n\\n---\\n\\n\".join(chunk_outputs)\n",
    "                )\n",
    "                final = process_with_gpt(combined_prompt)\n",
    "            else:\n",
    "                # Process entire document at once\n",
    "                final = extract_data_from_pdf_text(pdf_text, prompt)\n",
    "\n",
    "            results[pdf_path.name] = final\n",
    "            print(f\"✓ Successfully processed {pdf_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing {pdf_path.name}: {str(e)}\")\n",
    "            results[pdf_path.name] = f\"Error: {str(e)}\"\n",
    "\n",
    "    # Save results to file if specified\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for filename, data in results.items():\n",
    "                f.write(f\"\\n{'='*60}\\n\")\n",
    "                f.write(f\"File: {filename}\\n\")\n",
    "                f.write(f\"{'='*60}\\n\")\n",
    "                f.write(f\"{data}\\n\")\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your extraction prompt\n",
    "    PROMPT_TEMPLATE = \"\"\"\n",
    "    You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer.\n",
    "    I need your help reading some documents and extracting some information. I'm looking for information on the dataset the authors used. So things like:\n",
    "    * Title of the paper\n",
    "    * Authors of the paper\n",
    "    * Name of the dataset (collision or MC)\n",
    "    * Size in number of events\n",
    "    * Size in number of files\n",
    "    * Size in bytes\n",
    "    * Dataformat (AOD, miniAOD, nanoAOD, etc)\n",
    "    * Doi of datasets used\n",
    "    If the paper does not give exact numbers, provide an approximation and tag it as an approximation.\n",
    "    Look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper. Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\n",
    "    Produce a single CSV-style row (or a CSV with one header row + one data row) for each document with columns: Title, Authors, Dataset name (collision or MC), Size (events), Size (files), Size (bytes), Data format, Dataset DOI.\n",
    "    If you are given a text chunk, insert it where {text_chunk} appears in this template. If not, the document text will be appended after this prompt for context.\n",
    "\n",
    "    Document Text:\n",
    "    \"{text_chunk}\"\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Process PDFs\n",
    "    results = process_multiple_pdfs(\n",
    "        pdf_directory=r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        output_file=\"extracted_results.txt\",\n",
    "        chunk_threshold=1500\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    for filename in results:\n",
    "        print(f\"- {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create csv file with result data (automate this later)\n",
    "#Should be 8 items in the list\n",
    "#chat chatGPT doesn't output this in proper so have to fix this manually for now\n",
    "\n",
    "initial_data = [\n",
    "    [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"]\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"CMS Run 2012 DoubleMuParked dimuon sample (8 TeV, collision) - AOD; likely Runs 2012B/2012C/2012D\",\"~1e8-1e9 (approximation); ~1e3-1.5e4 (approximation); ~1e13-1e14 (approximation)\",\"AOD\",\"Lookup required (not specified in paper)\"]\n",
    "    [\"Unveiling Time-Varying Signals of Ultralight Bosonic Dark Matter at Collider and Beam Dump Experiments\",\"Jinhui Guo; Yuxuan He; Jia Liu; Xiao-Ping Wang; Ke-Pan Xie\",\"Published spectra from BaBar; LHCb; NA48/2; APEX; HADES; KLOE; PHENIX; WASA; E774; E141; NA64 (phenomenology recast; no CMS Open Data)\",\"N/A\",\"N/A\",\"N/A\",\"N/A\",\"N/A\"]\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"CMS 2012 8 TeV MET primary datasets (Run2012A, Run2012B, Run2012C) with HLT_PFMET150 (collision)\",\"approximate: B+C ~4.3e7 total; overall O(1e7-1e8); approximate: O(1e3-1e4);approximate: O(1-10 TB)\",\"AOD\",\"requires lookup - per-dataset DOIs for /MET/Run2012A-22Jan2013-v1/AOD; /MET/Run2012B-22Jan2013-v1/AOD; /MET/Run2012C-22Jan2013-v1/AOD\"]\n",
    "    [\"Exploring Uncharted Soft Displaced Vertices in Open Data\",\"Haipeng An; Zhen Hu; Zhen Liu; Daneng Yang\",\"TTJets_HadronicMGDecays_8TeV-madgraph (MC - full detector simulation)\",\"approximate: unknown\",\"AODSIM\",\"requires lookup - CMS Open Data record\"]\n",
    "    [\"End-to-End Jet Classification of Quarks and Gluons with the CMS Open Data\",\"M. Andrews; J. Alison; S. An; B. Burkle; S. Gleyzer; M. Narain; M. Paulini; B. Poczos; E. Usai\",\"CMS 2012 Open Data simulated QCD dijet (Pythia6 Z2*, pThat 80-120 and 120-170) (MC)\",\"~933,206 used (subset)\",\"GEN-SIM-RECO (RECO)\",\"not specified\"]\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleElectron/Run2015D-08Jun2016-v1/AOD\",\"approx-90,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-4,500\",\"approx-5e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24103\"]\n",
    "    [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"collision - /SingleMuon/Run2015D-16Dec2015-v1/AOD\",\"approx-120,000,000 (full dataset); subset used across study: ~1.2e6 unlabeled (pre-training) and ~1e4 labeled (classification)\",\"approx-6,000\",\"approx-7e12\",\"AOD (source); MiniAOD used in analysis\",\"http://opendata.cern.ch/record/24102\"]\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"/DoubleMu/Run2011A-12Oct2013-v1/AOD (collision)\",\"approx 2.0e7 total; 6,241,576 analyzed; 2,155,900 after baseline\",\"approx 2000\",\"approx 6.0e12\",\"AOD\",\"unknown - please verify on CERN Open Data\"]\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYJetsToLL_M-50_TuneZ2_7TeV-madgraph-tauola (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"]\n",
    "    [\"Searching in CMS Open Data for Dimuon Resonances with Substantial Transverse Momentum\",\"Cari Cesarotti; Yotam Soreq; Matthew J. Strassler; Jesse Thaler; Wei Xue\",\"DYToMuMu_M-10To50_TuneZ2_7TeV-pythia6 (MC)\",\"unknown\",\"unknown\",\"unknown\",\"AODSIM\",\"unknown\"]\n",
    "\n",
    "]\n",
    "\n",
    "with open('outputTest.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
