{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefbe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env (if present)\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36450c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with orginial paper- jet substructure\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"jet_substructure_paper.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell ONCE\n",
    "!curl https://arxiv.org/pdf/2312.06909v1 -o pretraining_strat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12035234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying other paper\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "file = client.files.create(\n",
    "    file=open(\"pretraining_strat.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "myprompt = 'You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets.'\n",
    "myprompt += 'You are also very, very careful and a good explainer. '\n",
    "myprompt += 'I need your help reading some documents and extracting some information. '\n",
    "myprompt += \"I'm looking for information on the dataset the authors used. So things like \\n\"\n",
    "myprompt += \"* Title of the paper \\n\"\n",
    "myprompt += \"* Authors of the paper \\n\"\n",
    "myprompt += \"* Name of the dataset (collision or MC) \\n\"\n",
    "myprompt += \"* Size in number of events \\n\"\n",
    "myprompt += \"* Size in number of files \\n\"\n",
    "myprompt += \"* Size in bytes \\n\"\n",
    "myprompt += \"* Dataformat (AOD, miniAOD, nanoAOD, etc) \\n\"\n",
    "myprompt += \"* Doi of datasets used \\n\"\n",
    "myprompt += \"I just uploaded to you a pdf of one of these papers. Can you try to extract that information?\"\n",
    "myprompt += \"Note that if the paper does specify the exact size in number of events, approximation is fine, just indicate that it's an approximation.\"\n",
    "myprompt += \"look up the exact DOIs and sizes from the CMS Open Data records you cite if they are not included in the paper.\"\n",
    "myprompt += \"Do not use em dashes (—) in the csv, use regular hyphens (-) instead.\"\n",
    "myprompt += \"Can you also create a csv file with that information, with columns for each of the items above?\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    #\"text\": \"What is the title of this paper and who wrote it?\",\n",
    "                    \"text\": myprompt,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "print()\n",
    "print(f\"Time to process: {time.time() - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b95fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a sample csv file from the output from pretraining paper and jet substructure paper (find a way to automate just using chat response instead of manually copying)\n",
    "data = [\n",
    "         [\"Title\",\"Authors\",\"Dataset name (collision or MC)\",\"Size (events)\",\"Size (files)\",\"Size (bytes)\",\"Data format\",\"Dataset DOI\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleElectron primary dataset Run2015D-08Jun2016-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24103\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"CMS SingleMuon primary dataset Run2015D-16Dec2015-v1 AOD (collision)\",\"\",\"\",\"\",\"AOD\",\"http://opendata.cern.ch/record/24102\"],\n",
    "         [\"Pre-training strategy using real particle collision data for event classification in collider physics\",\"Tomoe Kishimoto; Masahiro Morinaga; Masahiko Saito; Junichi Tanaka\",\"Private MC: 2HDM signal plus SM ttbar background (MC)\",\"~1200000 (total across train, val, test)\",\"N/A\",\"N/A\",\"Delphes fast-sim ROOT files\",\"N/A\"],\n",
    "         [\"Jet Substructure Studies with CMS Open Data\",\"Aashish Tripathee; Wei Xue; Andrew Larkoski; Simone Marzani; Jesse Thaler\",\"CMS Open Data - Jet Primary Dataset (/Jet/Run2010B-Apr21ReReco-v1/AOD), pp collision data at 7 TeV\",\"20022826\",\"1664\",\"2000000000000\",\"AOD\",\"10.7483/OPENDATA.CMS.3S7F.2E9W\"],\n",
    "\n",
    "        \n",
    "     ]\n",
    "\n",
    "with open('output.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efeb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
