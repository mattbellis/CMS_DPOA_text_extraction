{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import langextract as lx\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up Google Gemini API Key \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.environ.get('LANGEXTRACT_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8734242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell once\n",
    "\n",
    "!curl https://arxiv.org/pdf/1704.05842 -o jet_substructure_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell once\n",
    "!curl https://arxiv.org/pdf/2107.11405 -o stable_diffusion_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13446619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for paper 1\n",
    "doc = pymupdf.open('jet_substructure_paper.pdf')\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26eded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for paper 2\n",
    "doc2 = pymupdf.open('stable_diffusion_paper.pdf')\n",
    "\n",
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = \"\"\n",
    "for page in doc:\n",
    "    text = page.get_text()\n",
    "    alltext += text + \"\\n\"\n",
    "\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34921f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = \"\"\n",
    "for page in doc2:\n",
    "    text = page.get_text()\n",
    "    alltext += text + \"\\n\"\n",
    "\n",
    "print(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49725d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a concise prompt\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "\n",
    "You are an expert extractor for experimental particle physics papers. Your job is to find and return all mentions of datasets and any numeric/size information about them. For each dataset mentioned in the input text produce one extraction object.\n",
    "\n",
    "Extraction goals (for each dataset mention):\n",
    "\n",
    "dataset_name: the dataset identifier or human name exactly as written in the text (examples: '/JetHT/Run2016B-03Feb2017-v1/AOD', 'Jet Primary Dataset', 'Dataset_X (DX-2020-v2)', 'Sample-Small').\n",
    "file_count: the number of files (normalized to integer). If the text says 'about 420 files' set file_count to 420 and approximate=true.\n",
    "number_of_events: number of events (normalized to integer). Parse commas, scientific notation, 'million', 'k', etc.\n",
    "disk_size_bytes: disk size normalized to an integer number of bytes when a size is given or can be inferred. Prefer bytes; also provide human_readable_disk_size exactly as written.\n",
    "compressed_uncompressed: text describing compressed vs uncompressed sizes if both are given (string or null).\n",
    "inferred: true if the value was computed/inferred from other numeric parameters in the text (e.g., computed from luminosity and cross-section when both are given); false if directly stated.\n",
    "inference_reason: short text explaining any inference or calculation performed (null if none).\n",
    "extraction_text: the exact short span from the input that supports this extraction (copy verbatim).\n",
    "confidence: a float 0.0 to 1.0 expressing your confidence (0.9+ for explicit exact numbers, 0.6 to 0.8 for approximate phrasing, 0.3â€“0.5 for inferred / weak evidence).\n",
    "notes: any clarifying notes (null if none).\n",
    "\n",
    "Rules:\n",
    "\n",
    "Return one object per dataset mention (if the paragraph mentions multiple datasets, return multiple objects).\n",
    "Always include the exact extraction_text (a short substring from the input that shows the dataset or numbers).\n",
    "Normalize numeric strings:\n",
    "'1.2 million' -> 1200000\n",
    "'2.5 x 10^5' or '2.5e5' -> 250000\n",
    "'1,234' -> 1234\n",
    "Normalize sizes to bytes using: KB=10^3, MB=10^6, GB=10^9, TB=10^12 (and GiB/GiB as 2^30 if you prefer; state the conversion in inference_reason). If only a human unit is present, compute bytes and set disk_size_bytes accordingly.\n",
    "If a disk size is not explicit but can be reliably inferred from other numbers present in the text, compute it, set inferred=true, and explain the calculation in inference_reason.\n",
    "If text uses approximate words ('about', '~', 'approximately', 'roughly'), set approximate=true in notes or attributes and lower confidence.\n",
    "If no dataset-specific numbers are present, return an empty list (i.e., []). Do not hallucinate.\n",
    "Keep extraction_text short (one sentence or clause) and exact.\n",
    "When multiple different numeric claims appear (e.g., 'raw size 3.6 TB; archived 900 GB'), provide them in compressed_uncompressed or notes, and include disk_size_bytes as the primary size if one is clearly primary (or null if ambiguous). Provide both human_readable and bytes when possible.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# 2. Provide a high-quality example to guide the model\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"Our jet substructure study is based on the Jet Primary Dataset [76], \"\n",
    "            \"which is a subset of the full open data release with events that pass a predefined set \"\n",
    "            \"of single-jet and multi-jet triggers. There are 1664 AOD files in the Jet Primary Dataset, corresponding to 20,022,826 events \"\n",
    "            \"and 2.0 Terabytes of disk space.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"file count\",\n",
    "                extraction_text=\"1664 AOD files\",\n",
    "                attributes={\"type\": \"count\"}, \n",
    "\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"data set\",\n",
    "                extraction_text=\"Jet Primary Dataset\",\n",
    "                attributes={\"type\": \"data set\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"disk space\",\n",
    "                extraction_text=\"2.0 Terabytes of disk space\",\n",
    "                attributes={\"type\": \"disk space\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"number of events\",\n",
    "                extraction_text=\"20,022,826 events\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    # Additional diverse examples to improve robustness\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"The analysis uses an integrated sample from /MuonEG/Run2017C-31Mar2018-v1/AOD with roughly 4.5e5 events distributed over 320 files, \"\n",
    "            \"with an on-disk footprint of approximately 210 GiB (compressed).\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"data set\",\n",
    "                extraction_text=\"/MuonEG/Run2017C-31Mar2018-v1/AOD\",\n",
    "                attributes={\"type\": \"data set\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"number of events\",\n",
    "                extraction_text=\"roughly 4.5e5 events\",\n",
    "                attributes={\"type\": \"count\", \"approx\": True},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"file count\",\n",
    "                extraction_text=\"320 files\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"disk space\",\n",
    "                extraction_text=\"approximately 210 GiB (compressed)\",\n",
    "                attributes={\"type\": \"disk space\", \"units\": \"GiB\"},\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"We processed 1.25 million events from Dataset_Z_v3 (internal name: DZ_v3) across 1,800 AOD files. The raw size was ~2.8 TB, reduced to ~700 GB after compression and archiving.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"data set\",\n",
    "                extraction_text=\"Dataset_Z_v3 (internal name: DZ_v3)\",\n",
    "                attributes={\"type\": \"data set\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"number of events\",\n",
    "                extraction_text=\"1.25 million events\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"file count\",\n",
    "                extraction_text=\"1,800 AOD files\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"disk space\",\n",
    "                extraction_text=\"raw size was ~2.8 TB, reduced to ~700 GB after compression and archiving\",\n",
    "                attributes={\"type\": \"disk space\"},\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"The study analyzes a small specialized subset (Sample-Small) containing 2,340 events in 12 files; total disk usage is negligible (~15 MB) because the files are skimmed and only selected branches are kept.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"data set\",\n",
    "                extraction_text=\"Sample-Small\",\n",
    "                attributes={\"type\": \"data set\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"number of events\",\n",
    "                extraction_text=\"2,340 events\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"file count\",\n",
    "                extraction_text=\"12 files\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"disk space\",\n",
    "                extraction_text=\"~15 MB\",\n",
    "                attributes={\"type\": \"disk space\", \"units\": \"MB\"},\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 3. Run the extraction on your input text\n",
    "input_text = alltext\n",
    "\n",
    "\n",
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    api_key=api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and visualize the results\n",
    "lx.io.save_annotated_documents([result], output_name=\"jetsubstructureTest_extraction.jsonl\", output_dir=\".\")\n",
    "\n",
    "# Generate the interactive visualization\n",
    "html_content = lx.visualize(\"jetsubstructureTest_extraction.jsonl\")\n",
    "with open(\"jetsubstructureTest_extraction.html\", \"w\", encoding='utf-8') as f:\n",
    "    if hasattr(html_content, 'data'): #Check if the content has a data attribute\n",
    "        f.write(html_content.data)  \n",
    "    else:\n",
    "        f.write(html_content)\n",
    "\n",
    "print(\"Interactive visualization saved to jetsubstructureTest_extraction_visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cab9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying on a different paper\n",
    "# 1. Define a concise prompt\n",
    "prompt = textwrap.dedent(\"\"\"\\\n",
    "\n",
    "Extract mentions of names of datasets, size in file counts, number of events and disk space size in bytes from scientific texts.\n",
    "Use exact text for extractions. Do not paraphrase. \n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# 2. Provide a high-quality example to guide the model\n",
    "examples = [\n",
    "    lx.data.ExampleData(\n",
    "        text=(\n",
    "            \"Our jet substructure study is based on the Jet Primary Dataset [76], \"\n",
    "            \"which is a subset of the full open data release with events that pass a predefined set \"\n",
    "            \"of single-jet and multi-jet triggers. There are 1664 AOD files in the Jet Primary Dataset, corresponding to 20,022,826 events \"\n",
    "            \"and 2.0 Terabytes of disk space.\"\n",
    "        ),\n",
    "        extractions=[\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"file count\",\n",
    "                extraction_text=\"1664 AOD files\",\n",
    "                attributes={\"type\": \"count\"}, \n",
    "\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"data set\",\n",
    "                extraction_text=\"Jet Primary Dataset\",\n",
    "                attributes={\"type\": \"data set\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"disk space\",\n",
    "                extraction_text=\"2.0 Terabytes of disk space\",\n",
    "                attributes={\"type\": \"disk space\"},\n",
    "            ),\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"number of events\",\n",
    "                extraction_text=\"20,022,826 events\",\n",
    "                attributes={\"type\": \"count\"},\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Run the extraction on your input text\n",
    "input_text = alltext\n",
    "\n",
    "\n",
    "result = lx.extract(\n",
    "    text_or_documents=input_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and visualize the results\n",
    "lx.io.save_annotated_documents([result], output_name=\"stableDiffusionTest_extraction.jsonl\", output_dir=\".\")\n",
    "\n",
    "# Generate the interactive visualization\n",
    "html_content = lx.visualize(\"stableDiffusionTest_extraction.jsonl\")\n",
    "with open(\"stableDiffusionTest_extraction.html\", \"w\", encoding='utf-8') as f:\n",
    "    if hasattr(html_content, 'data'): #Check if the content has a data attribute\n",
    "        f.write(html_content.data)  \n",
    "    else:\n",
    "        f.write(html_content)\n",
    "\n",
    "print(\"Interactive visualization saved to stableDiffusionTest_extraction_visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a312dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better prompt?\n",
    "'''\n",
    "You are an expert extractor for experimental particle physics papers. Your job is to find and return all mentions of datasets and any numeric/size information about them. For each dataset mentioned in the input text produce one extraction object.\n",
    "\n",
    "Extraction goals (for each dataset mention):\n",
    "\n",
    "dataset_name: the dataset identifier or human name exactly as written in the text (examples: '/JetHT/Run2016B-03Feb2017-v1/AOD', 'Jet Primary Dataset', 'Dataset_X (DX-2020-v2)', 'Sample-Small').\n",
    "file_count: the number of files (normalized to integer). If the text says 'about 420 files' set file_count to 420 and approximate=true.\n",
    "number_of_events: number of events (normalized to integer). Parse commas, scientific notation, 'million', 'k', etc.\n",
    "disk_size_bytes: disk size normalized to an integer number of bytes when a size is given or can be inferred. Prefer bytes; also provide human_readable_disk_size exactly as written.\n",
    "compressed_uncompressed: text describing compressed vs uncompressed sizes if both are given (string or null).\n",
    "inferred: true if the value was computed/inferred from other numeric parameters in the text (e.g., computed from luminosity and cross-section when both are given); false if directly stated.\n",
    "inference_reason: short text explaining any inference or calculation performed (null if none).\n",
    "extraction_text: the exact short span from the input that supports this extraction (copy verbatim).\n",
    "confidence: a float 0.0 to 1.0 expressing your confidence (0.9+ for explicit exact numbers, 0.6 to 0.8 for approximate phrasing, 0.3â€“0.5 for inferred / weak evidence).\n",
    "notes: any clarifying notes (null if none).\n",
    "\n",
    "Rules:\n",
    "\n",
    "Return one object per dataset mention (if the paragraph mentions multiple datasets, return multiple objects).\n",
    "Always include the exact extraction_text (a short substring from the input that shows the dataset or numbers).\n",
    "Normalize numeric strings:\n",
    "'1.2 million' -> 1200000\n",
    "'2.5 x 10^5' or '2.5e5' -> 250000\n",
    "'1,234' -> 1234\n",
    "Normalize sizes to bytes using: KB=10^3, MB=10^6, GB=10^9, TB=10^12 (and GiB/GiB as 2^30 if you prefer; state the conversion in inference_reason). If only a human unit is present, compute bytes and set disk_size_bytes accordingly.\n",
    "If a disk size is not explicit but can be reliably inferred from other numbers present in the text, compute it, set inferred=true, and explain the calculation in inference_reason.\n",
    "If text uses approximate words ('about', '~', 'approximately', 'roughly'), set approximate=true in notes or attributes and lower confidence.\n",
    "If no dataset-specific numbers are present, return an empty list (i.e., []). Do not hallucinate.\n",
    "Keep extraction_text short (one sentence or clause) and exact.\n",
    "When multiple different numeric claims appear (e.g., 'raw size 3.6 TB; archived 900 GB'), provide them in compressed_uncompressed or notes, and include disk_size_bytes as the primary size if one is clearly primary (or null if ambiguous). Provide both human_readable and bytes when possible.\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
