{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0247c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Docstring for CMS_DPOA_text_extraction.claudeExtraction.ipynb\n",
    "\n",
    "Requirements:\n",
    "    pip install anthropic pypdf pdfplumber pandas\n",
    "\n",
    "Usage:\n",
    "    python extract_datasets_from_papers.py --input_folder ./papers --output dataset_info.csv\n",
    "    \n",
    "    # Or with API key as argument:\n",
    "    python extract_datasets_from_papers.py --input_folder ./papers --output dataset_info.csv --api_key sk-ant-...\n",
    "\n",
    "Environment:\n",
    "    Set ANTHROPIC_API_KEY environment variable or pass via --api_key argument\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import csv\n",
    "\n",
    "import anthropic\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Prompt template for Claude to extract dataset information\n",
    "EXTRACTION_PROMPT = \"\"\"You are a scientific data extraction assistant specializing in high-energy physics papers.\n",
    "\n",
    "Analyze the following physics paper text and extract ALL dataset information mentioned. Focus on:\n",
    "- CMS Open Data datasets\n",
    "- Monte Carlo simulation samples\n",
    "- Real collision data samples\n",
    "- Any datasets with DOIs or official citation paths\n",
    "\n",
    "For EACH dataset found, extract these fields (use \"null\" if not available):\n",
    "1. dataset_name: The name or identifier of the dataset\n",
    "2. dataset_type: \"Real Data\" or \"Simulated MC\" \n",
    "3. official_path: The official citation path (e.g., /Jet/Run2010B-Apr21ReReco-v1/AOD)\n",
    "4. events_total: Total number of events in the dataset\n",
    "5. events_used: Number of events actually used in the analysis\n",
    "6. collision_energy_tev: Center-of-mass energy in TeV\n",
    "7. generator: MC generator used (e.g., Pythia, Madgraph) or \"N/A (Real Data)\"\n",
    "8. doi: The DOI identifier (just the DOI, not the full URL)\n",
    "9. size_bytes: Dataset size in bytes (convert from TB, GB, MB if needed: 1TB=1e12, 1GB=1e9, 1MB=1e6). If multiple sizes given (e.g., raw vs compressed), use the original/raw size.\n",
    "10. notes: Any other important details (luminosity, run period, selection criteria, etc.)\n",
    "\n",
    "Return your response as a JSON array of objects. Each object represents one dataset.\n",
    "If the paper mentions multiple pT bins or variants of the same base dataset, list each separately.\n",
    "\n",
    "Example output format:\n",
    "[\n",
    "    {\n",
    "        \"dataset_name\": \"Jet Primary Dataset\",\n",
    "        \"dataset_type\": \"Real Data\",\n",
    "        \"official_path\": \"/Jet/Run2010B-Apr21ReReco-v1/AOD\",\n",
    "        \"events_total\": \"20022826\",\n",
    "        \"events_used\": \"768687\",\n",
    "        \"collision_energy_tev\": \"7\",\n",
    "        \"generator\": \"N/A (Real Data)\",\n",
    "        \"doi\": \"10.7483/OPENDATA.CMS.3S7F.2E9W\",\n",
    "        \"size_bytes\": \"2000000000000\",\n",
    "        \"notes\": \"Run 2010B, 31.8 pb-1 integrated luminosity, 2.0 TB original size\"\n",
    "    }\n",
    "]\n",
    "\n",
    "IMPORTANT: Return ONLY the JSON array, no other text or markdown formatting.\n",
    "\n",
    "Paper text to analyze:\n",
    "---\n",
    "{paper_text}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str, max_pages: Optional[int] = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        max_pages: Maximum number of pages to extract (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as a string\n",
    "    \"\"\"\n",
    "    logger.info(f\"Extracting text from: {pdf_path}\")\n",
    "    \n",
    "    text_parts = []\n",
    "    \n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            pages_to_process = pdf.pages[:max_pages] if max_pages else pdf.pages\n",
    "            \n",
    "            for i, page in enumerate(pages_to_process):\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text_parts.append(f\"--- Page {i+1} ---\\n{page_text}\")\n",
    "                    \n",
    "                # Also try to extract tables as they often contain dataset info\n",
    "                tables = page.extract_tables()\n",
    "                for j, table in enumerate(tables):\n",
    "                    if table:\n",
    "                        table_text = \"\\n\".join([\"\\t\".join([str(cell) if cell else \"\" for cell in row]) for row in table])\n",
    "                        text_parts.append(f\"--- Table {j+1} on Page {i+1} ---\\n{table_text}\")\n",
    "        \n",
    "        full_text = \"\\n\\n\".join(text_parts)\n",
    "        logger.info(f\"Extracted {len(full_text)} characters from {len(pages_to_process)} pages\")\n",
    "        return full_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_datasets_with_claude(\n",
    "    paper_text: str, \n",
    "    paper_name: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    model: str = \"claude-sonnet-4-20250514\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Use Claude API to extract dataset information from paper text.\n",
    "    \n",
    "    Args:\n",
    "        paper_text: The extracted text from the paper\n",
    "        paper_name: Name of the paper (for logging and output)\n",
    "        api_key: Anthropic API key (uses env var if not provided)\n",
    "        model: Claude model to use\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing dataset information\n",
    "    \"\"\"\n",
    "    logger.info(f\"Sending paper to Claude for analysis: {paper_name}\")\n",
    "    \n",
    "    # Initialize client\n",
    "    client = anthropic.Anthropic(api_key=api_key) if api_key else anthropic.Anthropic()\n",
    "    \n",
    "    # Truncate text if too long (keeping first and last parts for context)\n",
    "    max_chars = 150000  # Leave room for prompt and response\n",
    "    if len(paper_text) > max_chars:\n",
    "        half = max_chars // 2\n",
    "        paper_text = paper_text[:half] + \"\\n\\n[... middle section truncated ...]\\n\\n\" + paper_text[-half:]\n",
    "        logger.warning(f\"Paper text truncated to {max_chars} characters\")\n",
    "    \n",
    "    prompt = EXTRACTION_PROMPT.format(paper_text=paper_text)\n",
    "    \n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = message.content[0].text.strip()\n",
    "        \n",
    "        # Try to parse JSON response\n",
    "        # Handle case where response might have markdown code blocks\n",
    "        if response_text.startswith(\"```\"):\n",
    "            # Remove markdown code block formatting\n",
    "            lines = response_text.split(\"\\n\")\n",
    "            response_text = \"\\n\".join(lines[1:-1] if lines[-1] == \"```\" else lines[1:])\n",
    "        \n",
    "        datasets = json.loads(response_text)\n",
    "        \n",
    "        # Add paper name to each dataset\n",
    "        for dataset in datasets:\n",
    "            dataset[\"paper\"] = paper_name\n",
    "        \n",
    "        logger.info(f\"Extracted {len(datasets)} datasets from {paper_name}\")\n",
    "        return datasets\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse Claude response as JSON: {e}\")\n",
    "        logger.error(f\"Response was: {response_text[:500]}...\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling Claude API: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_papers_folder(\n",
    "    input_folder: str,\n",
    "    output_file: str,\n",
    "    api_key: Optional[str] = None,\n",
    "    model: str = \"claude-sonnet-4-20250514\",\n",
    "    max_pages: Optional[int] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all PDF papers in a folder and extract dataset information.\n",
    "    \n",
    "    Args:\n",
    "        input_folder: Path to folder containing PDF papers\n",
    "        output_file: Path for output CSV file\n",
    "        api_key: Anthropic API key\n",
    "        model: Claude model to use\n",
    "        max_pages: Maximum pages to process per PDF\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all extracted datasets\n",
    "    \"\"\"\n",
    "    input_path = Path(input_folder)\n",
    "    \n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input folder not found: {input_folder}\")\n",
    "    \n",
    "    # Find all PDF files\n",
    "    pdf_files = list(input_path.glob(\"*.pdf\")) + list(input_path.glob(\"*.PDF\"))\n",
    "    \n",
    "    if not pdf_files:\n",
    "        logger.warning(f\"No PDF files found in {input_folder}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    all_datasets = []\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            logger.info(f\"\\n{'='*60}\")\n",
    "            logger.info(f\"Processing: {pdf_file.name}\")\n",
    "            logger.info(f\"{'='*60}\")\n",
    "            \n",
    "            # Extract text from PDF\n",
    "            paper_text = extract_text_from_pdf(str(pdf_file), max_pages)\n",
    "            \n",
    "            # Get paper name (filename without extension)\n",
    "            paper_name = pdf_file.stem\n",
    "            \n",
    "            # Extract datasets using Claude\n",
    "            datasets = extract_datasets_with_claude(\n",
    "                paper_text=paper_text,\n",
    "                paper_name=paper_name,\n",
    "                api_key=api_key,\n",
    "                model=model\n",
    "            )\n",
    "            \n",
    "            all_datasets.extend(datasets)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {pdf_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_datasets:\n",
    "        logger.warning(\"No datasets extracted from any papers\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with consistent column order\n",
    "    columns = [\n",
    "        \"paper\",\n",
    "        \"dataset_name\", \n",
    "        \"dataset_type\",\n",
    "        \"official_path\",\n",
    "        \"events_total\",\n",
    "        \"events_used\",\n",
    "        \"collision_energy_tev\",\n",
    "        \"generator\",\n",
    "        \"doi\",\n",
    "        \"size_bytes\",\n",
    "        \"notes\"\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(all_datasets)\n",
    "    \n",
    "    # Ensure all columns exist\n",
    "    for col in columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"N/A\"\n",
    "    \n",
    "    # Reorder columns\n",
    "    df = df[columns]\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "    logger.info(f\"\\nSaved {len(df)} datasets to {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point for the script.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Extract dataset information from physics papers\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Examples:\n",
    "    # Process all PDFs in a folder\n",
    "    python extract_datasets_from_papers.py --input_folder ./papers --output datasets.csv\n",
    "    \n",
    "    # With explicit API key\n",
    "    python extract_datasets_from_papers.py --input_folder ./papers --output datasets.csv --api_key sk-ant-...\n",
    "    \n",
    "    # Limit pages processed per PDF (for faster processing)\n",
    "    python extract_datasets_from_papers.py --input_folder ./papers --output datasets.csv --max_pages 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--input_folder\", \"-i\",\n",
    "        default=r\"C:/Users/ejren/OneDrive/DPOA_papers\",\n",
    "        help=\"Path to folder containing PDF papers\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--output\", \"-o\",\n",
    "        default=\"dataset_info.csv\",\n",
    "        help=\"Output CSV file path (default: dataset_info.csv)\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--api_key\", \"-k\",\n",
    "        default=None,\n",
    "        help=\"Anthropic API key (or set ANTHROPIC_API_KEY env var)\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--model\", \"-m\",\n",
    "        default=\"claude-sonnet-4-20250514\",\n",
    "        help=\"Claude model to use (default: claude-sonnet-4-20250514)\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--max_pages\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Maximum pages to process per PDF (default: all)\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--verbose\", \"-v\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable verbose logging\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.verbose:\n",
    "        logging.getLogger().setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Check for API key\n",
    "    api_key = args.api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "    if not api_key:\n",
    "        logger.error(\"No API key provided. Set ANTHROPIC_API_KEY environment variable or use --api_key\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        df = process_papers_folder(\n",
    "            input_folder=args.input_folder,\n",
    "            output_file=args.output,\n",
    "            api_key=api_key,\n",
    "            model=args.model,\n",
    "            max_pages=args.max_pages\n",
    "        )\n",
    "        \n",
    "        if not df.empty:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"EXTRACTION COMPLETE\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Total datasets extracted: {len(df)}\")\n",
    "            print(f\"Papers processed: {df['paper'].nunique()}\")\n",
    "            print(f\"Output saved to: {args.output}\")\n",
    "            print(f\"\\nDataset types found:\")\n",
    "            print(df['dataset_type'].value_counts().to_string())\n",
    "            print(f\"\\nCollision energies:\")\n",
    "            print(df['collision_energy_tev'].value_counts().to_string())\n",
    "        else:\n",
    "            print(\"No datasets were extracted.\")\n",
    "            sys.exit(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
