{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import pymupdf\n",
    "import textwrap\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2b4b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path): \n",
    "    text = \"\"\n",
    "    with pymupdf.open(pdf_path) as doc: #open the pdf file\n",
    "        for page in doc: #iterate through each page\n",
    "            text += page.get_text() #extract text from each page and concatenate\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bbd63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split text into smaller chunks so ChatGPT doesn't lowkey crash out\n",
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31592196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to send chunks and extract data using GPT\n",
    "def extract_data_with_gpt(text_chunks, prompt_instructions):\n",
    "    extracted_data = []\n",
    "    for chunk in text_chunks:\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a data extraction assistant. Follow the user's instructions.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"{prompt_instructions}\\n\\nDocument text:\\n{chunk}\"}\n",
    "                ]\n",
    "            )\n",
    "            extracted_data.append(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with OpenAI API call: {e}\")\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perhaps this works-- ahahhahahahaha no it doesn't because paywall :(\n",
    "#Figure out later how to put all of the extracted data into a nice organized file\n",
    "\n",
    "# Define Extraction Logic\n",
    "pdf_file_path = \"jet_substructure_paper.pdf\" \n",
    "extraction_prompt = \"\"\"\n",
    "You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful.\n",
    "Extract the following information from the provided document text:\n",
    "1. Names of the datasets used (e.g., \"CMS Open Data\", \"ATLAS Open Data\").\n",
    "2. Size in the number of events of each dataset.\n",
    "3. The size in bytes of each dataset.\n",
    "4. The size in number of files of each dataset.\n",
    "\n",
    "If the paper gives a table of datasets, extract the information from the table. \n",
    "If the paper gives certain parameters for the datasets, use those to infer the size of the datasets.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute the Workflow\n",
    "document_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "if document_text:\n",
    "    chunks = chunk_text(document_text)\n",
    "    extracted_json_strings = extract_data_with_gpt(chunks, extraction_prompt)\n",
    "\n",
    "    # Combine results (assuming the main data to combine are line items)\n",
    "    all_extracted_items = []\n",
    "    for json_str in extracted_json_strings: \n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            if \"line_items\" in data and isinstance(data[\"line_items\"], list):\n",
    "                all_extracted_items.extend(data[\"line_items\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {e}\")\n",
    "\n",
    "    # Output\n",
    "    if all_extracted_items:\n",
    "        print(\"\\nSuccessfully extracted data:\")\n",
    "        print(json.dumps(all_extracted_items, indent=2))\n",
    "    else:\n",
    "        print(\"\\nCould not extract the requested data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
