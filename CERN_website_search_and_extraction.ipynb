{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a84615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf  # PyMuPDF\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "load_dotenv() #Load environment variables from .env file\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\") #Get the OpenAI API key from environment variable\n",
    "client = OpenAI(api_key=api_key) #Initialize OpenAI client\n",
    "\n",
    "#Function 1: Convert PDF pages to images\n",
    "def pdf_to_images(pdf_path, output_folder=\"pdf_images\"):\n",
    "    Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    \n",
    "    image_paths = []\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        pix = page.get_pixmap(matrix=pymupdf.Matrix(2, 2))\n",
    "        image_path = f\"{output_folder}/page_{page_num + 1}.png\"\n",
    "        pix.save(image_path)\n",
    "        image_paths.append(image_path)\n",
    "    \n",
    "    doc.close()\n",
    "    return image_paths\n",
    "\n",
    "#Function 2: Encode image to base64 because OpenAI API requires images in base64 format\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode image to base64\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "#Function 3: Extract CERN open data fields from PDF images using OpenAI API\n",
    "def extract_cern_data(image_paths, max_pages=None):\n",
    "    \n",
    "    if max_pages:\n",
    "        image_paths = image_paths[:max_pages]\n",
    "    \n",
    "    prompt = \"\"\"You are an expert at high energy particle physics and you understand jargon like \"events\" and datasets. You are also very, very careful and a good explainer.\n",
    "I need your help reading documents and extracting information about datasets used.\n",
    "\n",
    "Extract the following information:\n",
    "* Title of the paper\n",
    "* Authors of the paper (list all authors separated by semicolons, NO commas between authors)\n",
    "* Date of publication in YYYY-MM format (e.g., 2023-06)\n",
    "* Dataset name (collision or MC) - be specific, include year and collision energy if mentioned\n",
    "* Size in number of events (as a plain integer, no commas or scientific notation)\n",
    "* Size in number of files (as a plain integer, no commas)\n",
    "* Size in bytes (as a plain integer, no commas or units - convert GB/TB/PB to bytes)\n",
    "* Data format (AOD, miniAOD, nanoAOD, etc)\n",
    "* DOI of datasets used (full DOI string)\n",
    "\n",
    "Important guidelines:\n",
    "- For authors: Use semicolons as separators, NOT commas. Example: \"John Smith; Jane Doe; Bob Johnson\"\n",
    "- Do NOT use \"and\" before the last author - use semicolons consistently throughout\n",
    "- If exact numbers are not given, provide your best approximation without noting it as such\n",
    "- Convert all units to the requested format (e.g., \"1.5 TB\" becomes \"1500000000000\" bytes)\n",
    "- If scientific notation is used (e.g., \"10^6 events\"), convert to plain integer\n",
    "- Look carefully at tables, captions, methodology sections, and references for dataset information\n",
    "- If multiple datasets are used, create separate rows for each\n",
    "- Use regular hyphens (-) not em dashes (—)\n",
    "\n",
    "Return ONLY a CSV with this exact header:\n",
    "Title,Authors,Date,Dataset name (collision or MC),Size (events),Size (files),Size (bytes),Data format,Dataset DOI\n",
    "\n",
    "Then one or more data rows with the extracted values. Use \"null\" for unknown values.\n",
    "NO explanations, NO markdown formatting, NO code blocks, JUST the CSV.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(f\"    Encoding {len(image_paths)} images...\")\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        try:\n",
    "            base64_image = encode_image(img_path)\n",
    "            messages[0][\"content\"].append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{base64_image}\",\n",
    "                    \"detail\": \"high\"\n",
    "                }\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error encoding image {img_path}: {e}\")\n",
    "    \n",
    "    print(f\"    Sending request to OpenAI...\")\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=4000,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        print(f\"    Response received: {len(content)} characters\")\n",
    "        return content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ API Error: {e}\")\n",
    "        return None\n",
    "\n",
    "#Function 4: Search CERN Open Data website for dataset information using dataset name or DOI to fill missing fields\n",
    "def search_cern_website(dataset_name=None, doi=None):\n",
    "    #returns dictionary \n",
    "    print(f\"    Searching CERN website for: {dataset_name or doi}...\")\n",
    "    \n",
    "    try:\n",
    "        # Try specific dataset search if we have a name or DOI\n",
    "        if doi:\n",
    "            # Search by DOI\n",
    "            search_url = f\"https://opendata.cern.ch/search?q={doi}\"\n",
    "        elif dataset_name:\n",
    "            # Search by dataset name\n",
    "            search_url = f\"https://opendata.cern.ch/search?q={dataset_name}\"\n",
    "        else:\n",
    "            # General search\n",
    "            search_url = \"https://opendata.cern.ch/\"\n",
    "        \n",
    "        response = requests.get(search_url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        page_content = str(soup.body)\n",
    "        \n",
    "        # Use GPT to extract information from the page\n",
    "        prompt = f\"\"\"You are analyzing a page from the CERN Open Data website.\n",
    "Extract dataset information and return ONLY a valid JSON object (no markdown, no code blocks) with these fields:\n",
    "- dataset_name (string or null)\n",
    "- size_events (integer or null) - number of events\n",
    "- size_files (integer or null) - number of files  \n",
    "- size_bytes (integer or null) - size in bytes (convert any units)\n",
    "- data_format (string or null) - e.g., AOD, miniAOD, nanoAOD\n",
    "- dataset_doi (string or null) - full DOI\n",
    "\n",
    "If searching for specific dataset: {dataset_name or doi}\n",
    "Focus on finding information about this specific dataset if possible.\n",
    "\n",
    "Return ONLY the JSON object, nothing else.\n",
    "\n",
    "HTML content (first 15000 chars):\n",
    "{page_content[:15000]}\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You extract data from HTML and return only valid JSON objects.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content.strip()\n",
    "        # Remove markdown code blocks if present\n",
    "        result = re.sub(r'^```json\\s*|\\s*```$', '', result, flags=re.MULTILINE).strip()\n",
    "        \n",
    "        data = json.loads(result)\n",
    "        print(f\"    ✓ Found data on CERN website\")\n",
    "        return data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"    ✗ JSON decode error: {e}\")\n",
    "        print(f\"    Response was: {result[:200]}...\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"    ✗ Error searching CERN website: {e}\")\n",
    "        return None\n",
    "\n",
    "#Function 5: Fill missing fields in extracted data using CERN website search results\n",
    "def fill_missing_fields(row_dict, cern_data):\n",
    "    \"\"\"\n",
    "    Fill in missing fields from CERN website data\n",
    "    row_dict: dictionary of extracted data from PDF\n",
    "    cern_data: dictionary from CERN website search\n",
    "    Returns: updated row_dict\n",
    "    \"\"\"\n",
    "    if not cern_data:\n",
    "        return row_dict\n",
    "    \n",
    "    # Map CERN data fields to CSV fields\n",
    "    field_mapping = {\n",
    "        'Size (events)': 'size_events',\n",
    "        'Size (files)': 'size_files',\n",
    "        'Size (bytes)': 'size_bytes',\n",
    "        'Data format': 'data_format',\n",
    "        'Dataset DOI': 'dataset_doi',\n",
    "        'Dataset name (collision or MC)': 'dataset_name'\n",
    "    }\n",
    "    \n",
    "    filled_count = 0\n",
    "    for csv_field, cern_field in field_mapping.items():\n",
    "        # If field is missing or null in PDF extraction\n",
    "        if row_dict.get(csv_field) in [None, 'null', '', 'NULL']:\n",
    "            if cern_data.get(cern_field):\n",
    "                row_dict[csv_field] = str(cern_data[cern_field])\n",
    "                filled_count += 1\n",
    "    \n",
    "    if filled_count > 0:\n",
    "        print(f\"    ✓ Filled {filled_count} missing field(s) from CERN website\")\n",
    "    \n",
    "    return row_dict\n",
    "\n",
    "#Function 6: Parse CSV string to list of dictionaries\n",
    "def parse_csv_to_dict(csv_content):\n",
    "    \n",
    "    lines = csv_content.strip().split('\\n')\n",
    "    if len(lines) < 2:\n",
    "        return []\n",
    "    \n",
    "    header = lines[0].split(',')\n",
    "    rows = []\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        if line.strip() and not line.strip().startswith('Title,'):\n",
    "            # Simple CSV parsing (may need improvement for complex cases)\n",
    "            values = line.split(',')\n",
    "            if len(values) == len(header):\n",
    "                row_dict = dict(zip(header, values))\n",
    "                rows.append(row_dict)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "#Function 7: Process all PDFs in a directory\n",
    "def process_directory(pdf_directory, output_csv=\"extracted_data.csv\", max_pages_per_pdf=10):\n",
    "    \n",
    "    pdf_files = glob.glob(f\"{pdf_directory}/*.pdf\")\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in {pdf_directory}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    print(f\"Processing max {max_pages_per_pdf} pages per PDF\\n\")\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    for i, pdf_path in enumerate(pdf_files, 1):\n",
    "        pdf_name = Path(pdf_path).stem\n",
    "        print(f\"[{i}/{len(pdf_files)}] Processing {Path(pdf_path).name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Convert PDF to images\n",
    "            image_folder = Path(\"pdf_images\") / pdf_name\n",
    "            image_paths = pdf_to_images(pdf_path, output_folder=str(image_folder))\n",
    "            print(f\"  Converted {len(image_paths)} pages\")\n",
    "            \n",
    "            # Extract data from PDF\n",
    "            result = extract_cern_data(image_paths, max_pages=max_pages_per_pdf)\n",
    "            \n",
    "            if result is None:\n",
    "                print(f\"  ✗ No response from API\")\n",
    "                continue\n",
    "            \n",
    "            # Clean result\n",
    "            clean_result = result.replace(\"```csv\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            if not clean_result or len(clean_result) < 50:\n",
    "                print(f\"  ⚠ Warning: Response too short or empty\")\n",
    "                continue\n",
    "            \n",
    "            # Parse CSV to dictionaries\n",
    "            rows = parse_csv_to_dict(clean_result)\n",
    "            \n",
    "            # For each row, check for missing fields and search CERN website\n",
    "            for row_dict in rows:\n",
    "                dataset_name = row_dict.get('Dataset name (collision or MC)')\n",
    "                dataset_doi = row_dict.get('Dataset DOI')\n",
    "                \n",
    "                # Check if any important fields are missing\n",
    "                missing_fields = [\n",
    "                    field for field in ['Size (events)', 'Size (files)', 'Size (bytes)', 'Data format', 'Dataset DOI']\n",
    "                    if row_dict.get(field) in [None, 'null', '', 'NULL']\n",
    "                ]\n",
    "                \n",
    "                if missing_fields and (dataset_name not in [None, 'null', ''] or dataset_doi not in [None, 'null', '']):\n",
    "                    print(f\"  Missing fields: {', '.join(missing_fields)}\")\n",
    "                    print(f\"  Attempting to fill from CERN website...\")\n",
    "                    \n",
    "                    # Search CERN website\n",
    "                    cern_data = search_cern_website(\n",
    "                        dataset_name=dataset_name if dataset_name not in ['null', ''] else None,\n",
    "                        doi=dataset_doi if dataset_doi not in ['null', ''] else None\n",
    "                    )\n",
    "                    \n",
    "                    # Fill missing fields\n",
    "                    row_dict = fill_missing_fields(row_dict, cern_data)\n",
    "                    \n",
    "                    # Small delay between searches\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                all_rows.append(row_dict)\n",
    "            \n",
    "            print(f\"  ✓ Extraction complete ({len(rows)} dataset(s))\\n\")\n",
    "            \n",
    "            # Delay between PDFs\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error processing {Path(pdf_path).name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print()\n",
    "            continue\n",
    "    \n",
    "    # Write combined results to CSV\n",
    "    if all_rows:\n",
    "        print(f\"\\nWriting {len(all_rows)} rows to {output_csv}...\")\n",
    "        \n",
    "        with open(output_csv, 'w', encoding='utf-8', newline='') as f:\n",
    "            if all_rows:\n",
    "                fieldnames = all_rows[0].keys()\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(all_rows)\n",
    "        \n",
    "        print(f\"\\n✓ All data saved to {output_csv}\")\n",
    "        print(f\"✓ Processed {len(pdf_files)} papers\")\n",
    "        print(f\"✓ Wrote {len(all_rows)} dataset rows\")\n",
    "    else:\n",
    "        print(\"\\n✗ No data extracted from any papers\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_directory = r\"C:/Users/ejren/OneDrive/DPOA_papers\"  \n",
    "    \n",
    "    # Process all PDFs with CERN website fallback\n",
    "    process_directory(pdf_directory, output_csv=\"cern_data_extracted.csv\", max_pages_per_pdf=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
